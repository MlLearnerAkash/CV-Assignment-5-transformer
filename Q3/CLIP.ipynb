{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c70c88",
   "metadata": {},
   "source": [
    "**Q1) Do the visual encoders have the same architectures**\n",
    "- No, CLIP does not use a single visual encoder architecture. Instead, it explores the use of both modified ResNet architectures (CNNs) and Vision Transformer architectures. These architectures differ fundamentally in their approach to processing visual information. ResNets rely on convolutional layers to extract features hierarchically, while ViTs treat an image as a sequence of patches and apply Transformer layers with self-attention mechanisms. The specific modifications made to the ResNet architecture within CLIP, such as the attention pooling, further distinguish it from a standard ResNet. The use of both types of architectures allows the CLIP model to leverage different strengths in visual representation learning.\n",
    "- It is important to note that the text encoder used in CLIP is a Transformer architecture, distinct from both the ResNet and Vision Transformer-based visual encoders.The text encoder takes text as input, which is first converted into a lower-cased byte pair encoding (BPE) representation, and outputs a textual feature representation. These modality-specific feature representations are then linearly projected into a shared multi-modal embedding space where their similarity is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a1b23",
   "metadata": {},
   "source": [
    "### Q2) ILSVRC: dataset setup\n",
    "- ImageNet's label hierarchy is based on the WordNet hierarchy\n",
    "- Each concept, mostly described by bunch of words, is called \"synony set\" is called \"synset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e31c4b",
   "metadata": {},
   "source": [
    "### Q3) Could grouping objects based on synsets lead to problems for visual recognition?\n",
    "- Grouping objects based on synsets can lead to problems of visual recognition.\n",
    "    - **Polysemy:** Without word context, models like CLIP would struggle to determine correct visual concepts. For example, ImageNet contains synsets for both construction cranes and birds that fly, both referred to as \"cranes\"\n",
    "    - **Varying Granularity:** Levels of granularity used used in ImageNet may not be optimal for recognition task.\n",
    "    \n",
    "    - **Hierarchical Overlap:** For certain tasks, like the image classification task in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), the 1000 selected synsets are chosen such that there is no hierarchical overlap between them (no synset is an ancestor of another within this subset). This suggests that directly using the full WordNet hierarchy, where broader synsets contain more specific ones, could lead to complications or ambiguities in classification tasks if not handled carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9153f5e",
   "metadata": {},
   "source": [
    "### Q4) Visual differences in same synset\n",
    "- **Variation in visual characterstics:** Objects within the same synset may exhibit differences in their appearancs due to factors such as style, material and colour or pose. e.g. visually similar synset like seals and seal otters mar come closer due to sysnset postulate.\n",
    "- **Differences in image context and background:** Image captured in same synset mat be captured in different environmenta dna context.\n",
    " - **Changes in scale, viewpoint and articular:** Objects in the same synset may depict different scales, viewpoints and sate of articulation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5fb7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28fe178e",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "691ca50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: absl-py in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting array-record (from tensorflow_datasets)\n",
      "  Downloading array_record-0.4.0-py38-none-any.whl.metadata (502 bytes)\n",
      "Requirement already satisfied: click in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (8.1.8)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath]>=0.9.0->tensorflow_datasets)\n",
      "  Downloading etils-1.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: numpy in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (1.24.4)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (6.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: termcolor in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: importlib-resources in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from tensorflow_datasets) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.13.1)\n",
      "Requirement already satisfied: zipp in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.8.30)\n",
      "Requirement already satisfied: six in /home/akash/miniconda3/envs/v10/lib/python3.8/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting absl-py (from tensorflow_datasets)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.3.0-py3-none-any.whl (126 kB)\n",
      "Downloading array_record-0.4.0-py38-none-any.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=8a8b623bf60f8d941abb2db2fd824c86e252c3184fd88e6ed5ac16fa71d18d5a\n",
      "  Stored in directory: /home/akash/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, toml, promise, googleapis-common-protos, etils, absl-py, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.1.0\n",
      "    Uninstalling absl-py-2.1.0:\n",
      "      Successfully uninstalled absl-py-2.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.13.1 requires numpy<=1.24.3,>=1.22, but you have numpy 1.24.4 which is incompatible.\n",
      "tensorflow 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 array-record-0.4.0 dm-tree-0.1.8 etils-1.3.0 googleapis-common-protos-1.70.0 promise-2.3 tensorflow-metadata-1.14.0 tensorflow_datasets-4.9.2 toml-0.10.2\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5a8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.datasets import ImageNet\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f877ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CLIPClassifier:\n",
    "    def __init__(self, imagenet_classes, model_type=\"transformer\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.imagenet_classes = imagenet_classes\n",
    "        self.text_inputs = torch.cat([\n",
    "            clip.tokenize(f\"a photo of a {c}\") for c in imagenet_classes\n",
    "        ]).to(self.device)\n",
    "        if model_type == \"transformer\":\n",
    "            self.model, self.preprocess = clip.load(\"ViT-B/32\", self.device)\n",
    "        elif model_type == \"rn50\":\n",
    "            self.model, self.preprocess = clip.load(\"RN50\", self.device)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'transformer' or 'rn50'\")\n",
    "        self.model_type = model_type\n",
    "    \n",
    "    def classify_image(self, image_path):\n",
    "        image = self.preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(image)\n",
    "            text_features = self.model.encode_text(self.text_inputs)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        logit_scale = self.model.logit_scale.exp()\n",
    "        logits = logit_scale * (image_features @ text_features.T)\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        return probs.detach().cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0367b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_classes = [\"tench\", \"English springer\", \"cassette player\", \"chain saw\", \"church\", \"French horn\", \n",
    "                    \"garbage truck\", \"gas pump\", \"golf ball\", \"parachute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16683c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vit_classifier = CLIPClassifier(imagenet_classes, model_type=\"transformer\")\n",
    "rn50_classifier = CLIPClassifier(imagenet_classes, model_type=\"rn50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "458235b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q3/ImageNetData/imagenette2/train/n03425413/ILSVRC2012_val_00005183.JPEG\"\n",
    "\n",
    "image_path = \"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q3/ImageNetData/imagenette2/val/n03394916/n03394916_29142.JPEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34ddff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "French horn               99.95%\n",
      "garbage truck             0.02%\n",
      "church                    0.01%\n",
      "cassette player           0.01%\n",
      "English springer          0.00%\n",
      "Top 5 predictions:\n",
      "French horn               92.63%\n",
      "cassette player           3.54%\n",
      "church                    1.40%\n",
      "parachute                 0.87%\n",
      "English springer          0.57%\n"
     ]
    }
   ],
   "source": [
    "# Get top predictions\n",
    "probs = vit_classifier.classify_image(image_path)\n",
    "top_indices = probs.argsort()[0][-5:][::-1]\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{imagenet_classes[idx]:<25} {probs[0][idx]*100:.2f}%\")\n",
    "\n",
    "probs = rn50_classifier.classify_image(image_path)\n",
    "top_indices = probs.argsort()[0][-5:][::-1]\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{imagenet_classes[idx]:<25} {probs[0][idx]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d442c2",
   "metadata": {},
   "source": [
    "### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d3dda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "\n",
    "class CLIPClassifierFP16:\n",
    "    def __init__(self, imagenet_classes, model_type=\"transformer\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.imagenet_classes = imagenet_classes\n",
    "        # Tokenize text prompts\n",
    "        self.text_inputs = torch.cat([\n",
    "            clip.tokenize(f\"a photo of a {c}\") for c in imagenet_classes\n",
    "        ]).to(self.device)\n",
    "        # Load model\n",
    "        if model_type == \"transformer\":\n",
    "            self.model, self.preprocess = clip.load(\"ViT-B/32\", self.device)\n",
    "        elif model_type == \"rn50\":\n",
    "            self.model, self.preprocess = clip.load(\"RN50\", self.device)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'transformer' or 'rn50'\")\n",
    "        # Convert model to FP16\n",
    "        self.model.half()\n",
    "        assert self.model.dtype == torch.half , \"model is of fp16\"\n",
    "\n",
    "    def classify_image(self, image_path):\n",
    "        # Preprocess image and convert to FP16\n",
    "        image = self.preprocess(Image.open(image_path)).unsqueeze(0).to(self.device).half()\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                image_features = self.model.encode_image(image)\n",
    "                text_features = self.model.encode_text(self.text_inputs)\n",
    "            # image_features = self.model.encode_image(image)\n",
    "            # text_features = self.model.encode_text(self.text_inputs).to(image_features.dtype)\n",
    "        # Normalize features\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        # Compute logits\n",
    "        logit_scale = self.model.logit_scale.exp()\n",
    "        logits = logit_scale * (image_features @ text_features.T)\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        return probs.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5dd2ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vit_classifier16 = CLIPClassifierFP16(imagenet_classes, model_type=\"transformer\")\n",
    "rn50_classifier16 = CLIPClassifierFP16(imagenet_classes, model_type=\"rn50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9800de1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "French horn               99.95%\n",
      "garbage truck             0.02%\n",
      "church                    0.01%\n",
      "cassette player           0.01%\n",
      "English springer          0.00%\n",
      "==========\n",
      "Top 5 predictions:\n",
      "French horn               92.87%\n",
      "cassette player           3.41%\n",
      "church                    1.34%\n",
      "parachute                 0.83%\n",
      "English springer          0.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2877787/3126136429.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "# Get top predictions\n",
    "probs = vit_classifier16.classify_image(image_path)\n",
    "top_indices = probs.argsort()[0][-5:][::-1]\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{imagenet_classes[idx]:<25} {probs[0][idx]*100:.2f}%\")\n",
    "\n",
    "print(\"=\"*10)\n",
    "probs = rn50_classifier16.classify_image(image_path)\n",
    "top_indices = probs.argsort()[0][-5:][::-1]\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{imagenet_classes[idx]:<25} {probs[0][idx]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34a442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38341c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
