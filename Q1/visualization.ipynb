{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e491fba0",
   "metadata": {},
   "source": [
    "### Dino attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28674e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head0.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head1.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head2.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head3.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head4.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head5.png saved.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python dino/visualize_attention.py --image_path /home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/sample_images/bird.png --output_dir /home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2426859",
   "metadata": {},
   "source": [
    "# Visualizing Attention Heads for a deer image\n",
    "\n",
    "\n",
    "Below are the attention maps for each head. The images are displayed side by side for easy comparison.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; flex-wrap: nowrap;\">\n",
    "<div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/img.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head0.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head1</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head1.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 1\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head2</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head2.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 2\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head3</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head3.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 3\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head4</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head4.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 4\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head5</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head5.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 5\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1081be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256c315",
   "metadata": {},
   "source": [
    "# Visualizing Attention Heads for a bird image\n",
    "\n",
    "\n",
    "Below are the attention maps for each head. The images are displayed side by side for easy comparison.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; flex-wrap: nowrap;\">\n",
    "<div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/img.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head0.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head1</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head1.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 1\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head2</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head2.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 2\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head3</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head3.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 3\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head4</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head4.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 4\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head5</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head5.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 5\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a319331",
   "metadata": {},
   "source": [
    "### ViT CIFAR-10 Attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc9dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import yaml\n",
    "from vit_model import VisionTransformer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12006757",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vit_config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Assign the config values to the corresponding variables\n",
    "d_model   = config[\"d_model\"]\n",
    "n_classes = config[\"n_classes\"]\n",
    "img_size  = config[\"img_size\"]\n",
    "patch_size = config[\"patch_size\"]\n",
    "n_channels = config[\"n_channels\"]\n",
    "n_heads   = config[\"n_heads\"]\n",
    "n_layers  = config[\"n_layers\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "epochs    = config[\"epochs\"]\n",
    "alpha     = config[\"alpha\"]\n",
    "pos_type = config[\"pos_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600aa01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "model = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, pos_type=pos_type).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f40b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "  # T.Resize(img_size),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "test_set = CIFAR10(\n",
    "  root=\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dataset\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d1c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3466718/2343868849.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/vit-patchsize-8-attention_head-10-layer-10-augmentation-basic-pos_type-sinusoidal/best.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention statistics for image 1:\n",
      "Last layer attention shape: (10, 17, 17)\n",
      "Head 1: min=0.0132, max=0.1437, mean=0.0615\n",
      "Head 2: min=0.0058, max=0.1304, mean=0.0621\n",
      "Head 3: min=0.0028, max=0.3955, mean=0.0623\n",
      "Head 4: min=0.0006, max=0.4306, mean=0.0593\n",
      "Head 5: min=0.0024, max=0.2602, mean=0.0581\n",
      "Head 6: min=0.0018, max=0.1871, mean=0.0491\n",
      "Head 7: min=0.0001, max=0.2377, mean=0.0623\n",
      "Head 8: min=0.0496, max=0.0727, mean=0.0603\n",
      "Head 9: min=0.0150, max=0.2051, mean=0.0526\n",
      "Head 10: min=0.0078, max=0.1178, mean=0.0605\n",
      "Average attention: min=0.0342, max=0.0963, mean=0.0588\n",
      "Attention maps for image 1 saved to attention_maps/\n",
      "Saved files:\n",
      "- attention_maps/all_heads_last_layer_img1.png (17.7 KB)\n",
      "- attention_maps/avg_attention_last_layer_img1.png (9.9 KB)\n",
      "- attention_maps/attention_across_layers_img1.png (16.1 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def visualize_attention(model, image, image_idx, save_dir='attention_maps', n_heads=3, n_layers= 2):\n",
    "    # Create directory for saving attention maps\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.train(False)\n",
    "\n",
    "    # Forward pass to get attention weights\n",
    "    _ = model(image)\n",
    "\n",
    "    # Get number of patches in each dimension\n",
    "    n_patches_h = img_size[0] // patch_size[0]\n",
    "    n_patches_w = img_size[1] // patch_size[1]\n",
    "\n",
    "    # Original image for reference\n",
    "    img = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
    "\n",
    "    # Visualize attention from the last layer\n",
    "    last_layer = model.transformer_encoder[-1]\n",
    "    attn_weights = last_layer.mha.attn_weights.cpu().numpy()[0]  # [n_heads, seq_len, seq_len]\n",
    "\n",
    "    # Plot attention from [CLS] token to patch tokens for each head in the last layer\n",
    "    fig, axes = plt.subplots(1, n_heads, figsize=(15, 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for h in range(n_heads):\n",
    "        # Get attention weights from [CLS] token (index 0) to all patch tokens\n",
    "        cls_attn = attn_weights[h, 0, 1:]  # Skip the [CLS] token itself\n",
    "\n",
    "        # Reshape to match the image patches\n",
    "        attn_map = cls_attn.reshape(n_patches_h, n_patches_w)\n",
    "        # Plot\n",
    "        axes[h].imshow(img)\n",
    "        im = axes[h].imshow(attn_map, alpha=0.5, cmap='hot')\n",
    "        axes[h].set_title(f'Head {h+1}')\n",
    "        axes[h].axis('off')\n",
    "\n",
    "    # Save the figure with all heads\n",
    "    fig.suptitle(f'Attention Maps from [CLS] Token (Last Layer) - Image {image_idx}')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{save_dir}/all_heads_last_layer_img{image_idx}.png')\n",
    "\n",
    "    # Average attention across all heads\n",
    "    avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
    "    avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
    "\n",
    "    # Plot average attention\n",
    "    fig_avg = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
    "    plt.title('Average Attention (Last Layer)')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{save_dir}/avg_attention_last_layer_img{image_idx}.png')\n",
    "    plt.close(fig_avg)\n",
    "\n",
    "    # Visualize attention across all layers\n",
    "    fig_layers = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for l, encoder in enumerate(model.transformer_encoder):\n",
    "        attn_weights = encoder.mha.attn_weights.cpu().numpy()[0]\n",
    "\n",
    "        # Average attention across all heads for this layer\n",
    "        avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
    "        avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
    "\n",
    "        # Plot\n",
    "        plt.subplot(1, n_layers, l+1)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
    "        plt.title(f'Layer {l+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f'Attention Maps Across Layers - Image {image_idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/attention_across_layers_img{image_idx}.png')\n",
    "    plt.close(fig_layers)\n",
    "\n",
    "    # Close the first figure\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Print statistics about the attention maps for verification\n",
    "    print(f\"\\nAttention statistics for image {image_idx}:\")\n",
    "    print(f\"Last layer attention shape: {attn_weights.shape}\")\n",
    "\n",
    "    # Print min, max, mean values for each head in the last layer\n",
    "    for h in range(n_heads):\n",
    "        head_attn = attn_weights[h, 0, 1:]\n",
    "        print(f\"Head {h+1}: min={head_attn.min():.4f}, max={head_attn.max():.4f}, mean={head_attn.mean():.4f}\")\n",
    "\n",
    "    # Print average attention statistics\n",
    "    print(f\"Average attention: min={avg_attn.min():.4f}, max={avg_attn.max():.4f}, mean={avg_attn.mean():.4f}\")\n",
    "\n",
    "    print(f\"Attention maps for image {image_idx} saved to {save_dir}/\")\n",
    "\n",
    "    # List the saved files\n",
    "    saved_files = [\n",
    "        f'{save_dir}/all_heads_last_layer_img{image_idx}.png',\n",
    "        f'{save_dir}/avg_attention_last_layer_img{image_idx}.png',\n",
    "        f'{save_dir}/attention_across_layers_img{image_idx}.png'\n",
    "    ]\n",
    "\n",
    "    print(\"Saved files:\")\n",
    "    for file in saved_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"- {file} ({os.path.getsize(file) / 1024:.1f} KB)\")\n",
    "        else:\n",
    "            print(f\"- {file} (File not created)\")\n",
    "\n",
    "\n",
    "    return saved_files\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, pos_type=pos_type).to(device)\n",
    "    state_dict = torch.load(\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/vit-patchsize-8-attention_head-10-layer-10-augmentation-basic-pos_type-sinusoidal/best.pt\", map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            break\n",
    "    visualize_attention(model,inputs[0].unsqueeze(0), 1, n_layers = n_layers, n_heads = n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce14910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3466718/3605798690.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/vit-patchsize-8-attention_head-10-layer-10-augmentation-basic-pos_type-sinusoidal/best.pt', map_location=device)\n",
      "/tmp/ipykernel_3466718/3605798690.py:76: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined attention figure to attention_maps/attention_combined_img1.png\n"
     ]
    }
   ],
   "source": [
    "#Combining all the above code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_attention_combined(model, image, image_idx,\n",
    "                                 save_path='attention_combined.png',\n",
    "                                 n_heads=3, n_layers=2,\n",
    "                                 img_size=(224, 224), patch_size=(16, 16)):\n",
    "    # Ensure model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Forward pass to populate attention\n",
    "    _ = model(image)\n",
    "\n",
    "    # Number of patches in each dim\n",
    "    n_patches_h = img_size[0] // patch_size[0]\n",
    "    n_patches_w = img_size[1] // patch_size[1]\n",
    "\n",
    "    # Denormalize and prepare image\n",
    "    img = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "\n",
    "    # Collect attention weights per layer\n",
    "    all_layer_attns = []\n",
    "    for encoder in model.transformer_encoder:\n",
    "        w = encoder.mha.attn_weights.cpu().detach().numpy()[0]  # [heads, seq_len, seq_len]\n",
    "        all_layer_attns.append(w)\n",
    "\n",
    "    # Last-layer weights\n",
    "    last_attn = all_layer_attns[-1]\n",
    "\n",
    "    # Create big figure with GridSpec and spacing to avoid overlap\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, n_heads, wspace=0.3, hspace=0.4)\n",
    "\n",
    "    # 1) Row 0: Last-layer, individual heads\n",
    "    for h in range(n_heads):\n",
    "        ax = fig.add_subplot(gs[0, h])\n",
    "        cls_attn = last_attn[h, 0, 1:]\n",
    "        attn_map = cls_attn.reshape(n_patches_h, n_patches_w)\n",
    "        ax.imshow(img, extent=(0, n_patches_w, n_patches_h, 0))\n",
    "        im = ax.imshow(attn_map, alpha=0.8, cmap='hot', vmin=0, vmax=1, extent=(0, n_patches_w, n_patches_h, 0))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Head {h+1}')\n",
    "        # Add colorbar in range 0-1\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04, ticks=[0, 0.5, 1])\n",
    "\n",
    "    # 2) Row 1: Average attention (last layer)\n",
    "    ax_avg = fig.add_subplot(gs[1, :])\n",
    "    avg_last = last_attn.mean(axis=0)[0, 1:].reshape(n_patches_h, n_patches_w)\n",
    "    ax_avg.imshow(img, extent=(0, n_patches_w, n_patches_h, 0))\n",
    "    im_avg = ax_avg.imshow(avg_last, alpha=0.8, cmap='hot', vmin=0, vmax=1, extent=(0, n_patches_w, n_patches_h, 0))\n",
    "    ax_avg.axis('off')\n",
    "    ax_avg.set_title('Average Attention (Last Layer)')\n",
    "    fig.colorbar(im_avg, ax=ax_avg, orientation='horizontal', fraction=0.05, pad=0.1, ticks=[0, 0.5, 1])\n",
    "\n",
    "    # 3) Row 2: Across layers, averaged heads per layer\n",
    "    for l in range(n_layers):\n",
    "        ax = fig.add_subplot(gs[2, l])\n",
    "        avg_layer = all_layer_attns[l].mean(axis=0)[0, 1:].reshape(n_patches_h, n_patches_w)\n",
    "        ax.imshow(img, extent=(0, n_patches_w, n_patches_h, 0))\n",
    "        im_layer = ax.imshow(avg_layer, alpha=0.8, cmap='hot', vmin=0, vmax=1, extent=(0, n_patches_w, n_patches_h, 0))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Layer {l+1}')\n",
    "        fig.colorbar(im_layer, ax=ax, fraction=0.046, pad=0.04, ticks=[0, 0.5, 1])\n",
    "\n",
    "    # 4) Original image inset moved above first row to avoid overlap\n",
    "    inset_ax = fig.add_axes([0.88, 0.88, 0.1, 0.1])  # [left, bottom, width, height]\n",
    "    inset_ax.imshow(img)\n",
    "    inset_ax.axis('off')\n",
    "    inset_ax.set_title('Original Image', fontsize=10)\n",
    "\n",
    "    # Overall title and save\n",
    "    fig.suptitle(f'Combined Attention Visualization - Image {image_idx}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined attention figure to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = VisionTransformer(d_model, n_classes, img_size, patch_size,\n",
    "                              n_channels, n_heads, n_layers,\n",
    "                              pos_type=pos_type).to(device)\n",
    "    ckpt = torch.load('/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/vit-patchsize-8-attention_head-10-layer-10-augmentation-basic-pos_type-sinusoidal/best.pt', map_location=device)\n",
    "    model.load_state_dict(ckpt)\n",
    "    \n",
    "    # grab a batch\n",
    "    inputs, _ = next(iter(test_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    visualize_attention_combined(model, inputs[0].unsqueeze(0),\n",
    "                                 image_idx=1,\n",
    "                                 save_path='attention_maps/attention_combined_img1.png',\n",
    "                                 n_heads=n_heads,\n",
    "                                 n_layers=n_layers,\n",
    "                                 img_size=img_size,\n",
    "                                 patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ed394",
   "metadata": {},
   "source": [
    "#### Attention rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(attention_maps, discard_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Compute attention rollout as described in the ViT paper and\n",
    "    'Quantifying Attention Flow in Transformers'\n",
    "    \n",
    "    Args:\n",
    "        attention_maps: List of attention maps from each layer [num_layers, batch, heads, seq_len, seq_len]\n",
    "        discard_ratio: The ratio of lowest attentions to discard\n",
    "        \n",
    "    Returns:\n",
    "        rollout: The aggregated attention map [batch, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    # Average the attention weights across heads\n",
    "    \n",
    "    attention_maps = [attn.mean(dim=1) for attn in attention_maps]  # [num_layers, batch, seq_len, seq_len]\n",
    "    \n",
    "    # Apply discard ratio: zero out the lowest values\n",
    "    \n",
    "    for i in range(len(attention_maps)):\n",
    "        flat = attention_maps[i].reshape(attention_maps[i].size(0), -1)\n",
    "        _, indices = flat.sort(dim=-1)\n",
    "        flat_indices = indices[:, :int(flat.size(-1) * discard_ratio)]\n",
    "        \n",
    "        # Create a mask with zeros at the discarded positions\n",
    "        \n",
    "        batch_size, seq_len = attention_maps[i].size(0), attention_maps[i].size(1)\n",
    "        mask = torch.ones(batch_size, seq_len, seq_len, device=attention_maps[i].device)\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            row_indices = flat_indices[j] // seq_len\n",
    "            col_indices = flat_indices[j] % seq_len\n",
    "            mask[j, row_indices, col_indices] = 0\n",
    "            \n",
    "        attention_maps[i] = attention_maps[i] * mask\n",
    "        \n",
    "        # Re-normalize the attention maps\n",
    "        \n",
    "        attention_maps[i] = attention_maps[i] / (attention_maps[i].sum(dim=-1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    # Start with identity matrix\n",
    "    \n",
    "    batch_size, seq_len = attention_maps[0].size(0), attention_maps[0].size(1)\n",
    "    rollout = torch.eye(seq_len).unsqueeze(0).repeat(batch_size, 1, 1).to(attention_maps[0].device)\n",
    "    \n",
    "    # Compute rollout through matrix multiplication\n",
    "    \n",
    "    for attn in attention_maps:\n",
    "        rollout = torch.bmm(attn, rollout)\n",
    "    \n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b439813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
