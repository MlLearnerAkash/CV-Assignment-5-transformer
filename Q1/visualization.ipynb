{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e491fba0",
   "metadata": {},
   "source": [
    "### Dino attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28674e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head0.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head1.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head2.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head3.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head4.png saved.\n",
      "/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output/attn-head5.png saved.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python dino/visualize_attention.py --image_path /home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/sample_images/bird.png --output_dir /home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dino/attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2426859",
   "metadata": {},
   "source": [
    "# Visualizing Attention Heads for a deer image\n",
    "\n",
    "\n",
    "Below are the attention maps for each head. The images are displayed side by side for easy comparison.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; flex-wrap: nowrap;\">\n",
    "<div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/img.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head0.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head1</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head1.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 1\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head2</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head2.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 2\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head3</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head3.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 3\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head4</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head4.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 4\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head5</strong></div>\n",
    "    <img src=\"dino/attention_output/deer/attn-head5.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 5\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1081be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256c315",
   "metadata": {},
   "source": [
    "# Visualizing Attention Heads for a bird image\n",
    "\n",
    "\n",
    "Below are the attention maps for each head. The images are displayed side by side for easy comparison.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; flex-wrap: nowrap;\">\n",
    "<div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/img.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head0</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head0.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 0\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head1</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head1.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 1\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head2</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head2.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 2\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head3</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head3.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 3\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head4</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head4.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 4\">\n",
    "  </div>\n",
    "  <div style=\"margin: 10px; text-align: center;\">\n",
    "    <div><strong>attn-head5</strong></div>\n",
    "    <img src=\"dino/attention_output/bird/attn-head5.png\" style=\"max-width: 200px; height: auto;\" alt=\"Attention Head 5\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a319331",
   "metadata": {},
   "source": [
    "### ViT CIFAR-10 Attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc9dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import yaml\n",
    "from vit_model import VisionTransformer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12006757",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vit_config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Assign the config values to the corresponding variables\n",
    "d_model   = config[\"d_model\"]\n",
    "n_classes = config[\"n_classes\"]\n",
    "img_size  = config[\"img_size\"]\n",
    "patch_size = config[\"patch_size\"]\n",
    "n_channels = config[\"n_channels\"]\n",
    "n_heads   = config[\"n_heads\"]\n",
    "n_layers  = config[\"n_layers\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "epochs    = config[\"epochs\"]\n",
    "alpha     = config[\"alpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600aa01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "model = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f40b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "  # T.Resize(img_size),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "test_set = CIFAR10(\n",
    "  root=\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dataset\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98d1c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2883850/1095376233.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/vit-patchsize-2-attention_head-10-layer-10/best.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention statistics for image 1:\n",
      "Last layer attention shape: (10, 257, 257)\n",
      "Head 1: min=0.0012, max=0.0084, mean=0.0039\n",
      "Head 2: min=0.0028, max=0.0056, mean=0.0039\n",
      "Head 3: min=0.0016, max=0.0080, mean=0.0039\n",
      "Head 4: min=0.0000, max=0.0327, mean=0.0039\n",
      "Head 5: min=0.0029, max=0.0057, mean=0.0039\n",
      "Head 6: min=0.0001, max=0.0163, mean=0.0039\n",
      "Head 7: min=0.0025, max=0.0076, mean=0.0039\n",
      "Head 8: min=0.0025, max=0.0054, mean=0.0039\n",
      "Head 9: min=0.0000, max=0.0223, mean=0.0039\n",
      "Head 10: min=0.0003, max=0.0146, mean=0.0039\n",
      "Average attention: min=0.0019, max=0.0078, mean=0.0039\n",
      "Attention maps for image 1 saved to attention_maps/\n",
      "Saved files:\n",
      "- attention_maps/all_heads_last_layer_img1.png (31.4 KB)\n",
      "- attention_maps/avg_attention_last_layer_img1.png (11.5 KB)\n",
      "- attention_maps/attention_across_layers_img1.png (28.5 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def visualize_attention(model, image, image_idx, save_dir='attention_maps', n_heads=3, n_layers= 2):\n",
    "    # Create directory for saving attention maps\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.train(False)\n",
    "\n",
    "    # Forward pass to get attention weights\n",
    "    _ = model(image)\n",
    "\n",
    "    # Get number of patches in each dimension\n",
    "    n_patches_h = img_size[0] // patch_size[0]\n",
    "    n_patches_w = img_size[1] // patch_size[1]\n",
    "\n",
    "    # Original image for reference\n",
    "    img = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
    "\n",
    "    # Visualize attention from the last layer\n",
    "    last_layer = model.transformer_encoder[-1]\n",
    "    attn_weights = last_layer.mha.attn_weights.cpu().numpy()[0]  # [n_heads, seq_len, seq_len]\n",
    "\n",
    "    # Plot attention from [CLS] token to patch tokens for each head in the last layer\n",
    "    fig, axes = plt.subplots(1, n_heads, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for h in range(n_heads):\n",
    "        # Get attention weights from [CLS] token (index 0) to all patch tokens\n",
    "        cls_attn = attn_weights[h, 0, 1:]  # Skip the [CLS] token itself\n",
    "\n",
    "        # Reshape to match the image patches\n",
    "        attn_map = cls_attn.reshape(n_patches_h, n_patches_w)\n",
    "        # Plot\n",
    "        axes[h].imshow(img)\n",
    "        im = axes[h].imshow(attn_map, alpha=0.5, cmap='hot')\n",
    "        axes[h].set_title(f'Head {h+1}')\n",
    "        axes[h].axis('off')\n",
    "\n",
    "    # Save the figure with all heads\n",
    "    fig.suptitle(f'Attention Maps from [CLS] Token (Last Layer) - Image {image_idx}')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{save_dir}/all_heads_last_layer_img{image_idx}.png')\n",
    "\n",
    "    # Average attention across all heads\n",
    "    avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
    "    avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
    "\n",
    "    # Plot average attention\n",
    "    fig_avg = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
    "    plt.title('Average Attention (Last Layer)')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{save_dir}/avg_attention_last_layer_img{image_idx}.png')\n",
    "    plt.close(fig_avg)\n",
    "\n",
    "    # Visualize attention across all layers\n",
    "    fig_layers = plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for l, encoder in enumerate(model.transformer_encoder):\n",
    "        attn_weights = encoder.mha.attn_weights.cpu().numpy()[0]\n",
    "\n",
    "        # Average attention across all heads for this layer\n",
    "        avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
    "        avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
    "\n",
    "        # Plot\n",
    "        plt.subplot(1, n_layers, l+1)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
    "        plt.title(f'Layer {l+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f'Attention Maps Across Layers - Image {image_idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/attention_across_layers_img{image_idx}.png')\n",
    "    plt.close(fig_layers)\n",
    "\n",
    "    # Close the first figure\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Print statistics about the attention maps for verification\n",
    "    print(f\"\\nAttention statistics for image {image_idx}:\")\n",
    "    print(f\"Last layer attention shape: {attn_weights.shape}\")\n",
    "\n",
    "    # Print min, max, mean values for each head in the last layer\n",
    "    for h in range(n_heads):\n",
    "        head_attn = attn_weights[h, 0, 1:]\n",
    "        print(f\"Head {h+1}: min={head_attn.min():.4f}, max={head_attn.max():.4f}, mean={head_attn.mean():.4f}\")\n",
    "\n",
    "    # Print average attention statistics\n",
    "    print(f\"Average attention: min={avg_attn.min():.4f}, max={avg_attn.max():.4f}, mean={avg_attn.mean():.4f}\")\n",
    "\n",
    "    print(f\"Attention maps for image {image_idx} saved to {save_dir}/\")\n",
    "\n",
    "    # List the saved files\n",
    "    saved_files = [\n",
    "        f'{save_dir}/all_heads_last_layer_img{image_idx}.png',\n",
    "        f'{save_dir}/avg_attention_last_layer_img{image_idx}.png',\n",
    "        f'{save_dir}/attention_across_layers_img{image_idx}.png'\n",
    "    ]\n",
    "\n",
    "    print(\"Saved files:\")\n",
    "    for file in saved_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"- {file} ({os.path.getsize(file) / 1024:.1f} KB)\")\n",
    "        else:\n",
    "            print(f\"- {file} (File not created)\")\n",
    "\n",
    "    return saved_files\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
    "    state_dict = torch.load(\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/vit-patchsize-2-attention_head-10-layer-10/best.pt\", map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            break\n",
    "    visualize_attention(model,inputs[0].unsqueeze(0), 1, n_layers = n_layers, n_heads = n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce14910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
