{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39_bAtOyPoIA",
        "outputId": "e26e2f93-609a-423e-a5c5-d8cf521700dc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import os\n",
        "import yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Ref: [Building a Vision Transformer Model From Scratch](https://medium.com/correll-lab/building-a-vision-transformer-model-from-scratch-a3054f707cc6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5od6MAVPvMp"
      },
      "source": [
        "# Patch Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MOSovXJoPxws"
      },
      "outputs": [],
      "source": [
        "# class PatchEmbedding(nn.Module):\n",
        "#   def __init__(self, d_model, img_size, patch_size, n_channels):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.d_model = d_model # Dimensionality of Model\n",
        "#     self.img_size = img_size # Image Size\n",
        "#     self.patch_size = patch_size # Patch Size\n",
        "#     self.n_channels = n_channels # Number of Channels\n",
        "\n",
        "#     self.linear_project = nn.Conv2d(self.n_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "\n",
        "#   # B: Batch Size\n",
        "#   # C: Image Channels\n",
        "#   # H: Image Height\n",
        "#   # W: Image Width\n",
        "#   # P_col: Patch Column\n",
        "#   # P_row: Patch Row\n",
        "#   def forward(self, x):\n",
        "#     x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n",
        "\n",
        "#     x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n",
        "\n",
        "#     x = x.transpose(-2, -1) # (B, d_model, P) -> (B, P, d_model)\n",
        "\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6RwOLUuP0RE"
      },
      "source": [
        "# Class Token and Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KFrdTG6tP13V"
      },
      "outputs": [],
      "source": [
        "# class PositionalEncoding(nn.Module):\n",
        "#   def __init__(self, d_model, max_seq_length):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.cls_token = nn.Parameter(torch.randn(1, 1, d_model)) # Classification Token\n",
        "\n",
        "#     # Creating positional encoding\n",
        "#     pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "#     for pos in range(max_seq_length):\n",
        "#       for i in range(d_model):\n",
        "#         if i % 2 == 0:\n",
        "#           pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n",
        "#         else:\n",
        "#           pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n",
        "\n",
        "#     self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     # Expand to have class token for every image in batch\n",
        "#     tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n",
        "\n",
        "#     # Adding class tokens to the beginning of each embedding\n",
        "#     x = torch.cat((tokens_batch,x), dim=1)\n",
        "\n",
        "#     # Add positional encoding to embeddings\n",
        "#     x = x + self.pe\n",
        "\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN9_RMizP6TW"
      },
      "source": [
        "# Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3jQMJjrVP7El"
      },
      "outputs": [],
      "source": [
        "# class AttentionHead(nn.Module):\n",
        "#   def __init__(self, d_model, head_size):\n",
        "#     super().__init__()\n",
        "#     self.head_size = head_size\n",
        "\n",
        "#     self.query = nn.Linear(d_model, head_size)\n",
        "#     self.key = nn.Linear(d_model, head_size)\n",
        "#     self.value = nn.Linear(d_model, head_size)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     # Obtaining Queries, Keys, and Values\n",
        "#     Q = self.query(x)\n",
        "#     K = self.key(x)\n",
        "#     V = self.value(x)\n",
        "\n",
        "#     # Dot Product of Queries and Keys\n",
        "#     attention = Q @ K.transpose(-2,-1)\n",
        "\n",
        "#     # Scaling\n",
        "#     attention = attention / (self.head_size ** 0.5)\n",
        "\n",
        "#     attention = torch.softmax(attention, dim=-1)\n",
        "\n",
        "#     attention = attention @ V\n",
        "\n",
        "#     return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "myuSl3Y2P7ZT"
      },
      "outputs": [],
      "source": [
        "# class MultiHeadAttention(nn.Module):\n",
        "#   def __init__(self, d_model, n_heads):\n",
        "#     super().__init__()\n",
        "#     self.head_size = d_model // n_heads\n",
        "\n",
        "#     self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "#     self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     # Combine attention heads\n",
        "#     out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "#     out = self.W_o(out)\n",
        "\n",
        "#     return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClcjVdqUQMoB"
      },
      "source": [
        "# Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nR2pP8B9QOGY"
      },
      "outputs": [],
      "source": [
        "# class TransformerEncoder(nn.Module):\n",
        "#   def __init__(self, d_model, n_heads, r_mlp=4):\n",
        "#     super().__init__()\n",
        "#     self.d_model = d_model\n",
        "#     self.n_heads = n_heads\n",
        "\n",
        "#     # Sub-Layer 1 Normalization\n",
        "#     self.ln1 = nn.LayerNorm(d_model)\n",
        "\n",
        "#     # Multi-Head Attention\n",
        "#     self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "#     # Sub-Layer 2 Normalization\n",
        "#     self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "#     # Multilayer Perception\n",
        "#     self.mlp = nn.Sequential(\n",
        "#         nn.Linear(d_model, d_model*r_mlp),\n",
        "#         nn.GELU(),\n",
        "#         nn.Linear(d_model*r_mlp, d_model)\n",
        "#     )\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     # Residual Connection After Sub-Layer 1\n",
        "#     out = x + self.mha(self.ln1(x))\n",
        "\n",
        "#     # Residual Connection After Sub-Layer 2\n",
        "#     out = out + self.mlp(self.ln2(out))\n",
        "\n",
        "#     return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRlHsYkZQRNz"
      },
      "source": [
        "# Vision Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UDUqSKtwQTQs"
      },
      "outputs": [],
      "source": [
        "# class VisionTransformer(nn.Module):\n",
        "#   def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
        "#     super().__init__()\n",
        "\n",
        "#     assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
        "#     assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "#     self.d_model = d_model # Dimensionality of model\n",
        "#     self.n_classes = n_classes # Number of classes\n",
        "#     self.img_size = img_size # Image size\n",
        "#     self.patch_size = patch_size # Patch size\n",
        "#     self.n_channels = n_channels # Number of channels\n",
        "#     self.n_heads = n_heads # Number of attention heads\n",
        "\n",
        "#     self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
        "#     self.max_seq_length = self.n_patches + 1\n",
        "\n",
        "#     self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
        "#     self.positional_encoding = PositionalEncoding( self.d_model, self.max_seq_length)\n",
        "#     self.transformer_encoder = nn.Sequential(*[TransformerEncoder( self.d_model, self.n_heads) for _ in range(n_layers)])\n",
        "\n",
        "#     # Classification MLP\n",
        "#     self.classifier = nn.Sequential(\n",
        "#         nn.Linear(self.d_model, self.n_classes),\n",
        "#         nn.Softmax(dim=-1)\n",
        "#     )\n",
        "\n",
        "#   def forward(self, images):\n",
        "#     x = self.patch_embedding(images)\n",
        "\n",
        "#     x = self.positional_encoding(x)\n",
        "\n",
        "#     x = self.transformer_encoder(x)\n",
        "\n",
        "#     x = self.classifier(x[:,0])\n",
        "\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ap6w0EmQbMB"
      },
      "source": [
        "# Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zGxBtIXiQb1s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanna1\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.2s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/wandb/run-20250417_051425-u24nknic</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/manna1/vit-image-classification/runs/u24nknic' target=\"_blank\">vit-patchsize-2-attention_head-10-layer-10</a></strong> to <a href='https://wandb.ai/manna1/vit-image-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/manna1/vit-image-classification' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/manna1/vit-image-classification/runs/u24nknic' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification/runs/u24nknic</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# d_model = 20\n",
        "# n_classes = 10\n",
        "# img_size = (32,32)\n",
        "# patch_size = (2,2)\n",
        "# n_channels = 3\n",
        "# n_heads = 10\n",
        "# n_layers = 10\n",
        "# batch_size = 128\n",
        "# epochs = 1\n",
        "# alpha = 0.005\n",
        "\n",
        "with open(\"vit_config.yaml\", \"r\") as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "# Assign the config values to the corresponding variables\n",
        "d_model   = config[\"d_model\"]\n",
        "n_classes = config[\"n_classes\"]\n",
        "img_size  = config[\"img_size\"]\n",
        "patch_size = config[\"patch_size\"]\n",
        "n_channels = config[\"n_channels\"]\n",
        "n_heads   = config[\"n_heads\"]\n",
        "n_layers  = config[\"n_layers\"]\n",
        "batch_size = config[\"batch_size\"]\n",
        "epochs    = config[\"epochs\"]\n",
        "alpha     = config[\"alpha\"]\n",
        "\n",
        "exp_name = f\"vit-patchsize-{patch_size[0]}-attention_head-{n_heads}-layer-{n_layers}\"\n",
        "\n",
        "wandb.init(project = \"vit-image-classification\", name = exp_name)\n",
        "\n",
        "config = {\n",
        "    \"d_model\": d_model,\n",
        "    \"n_classes\": n_classes,\n",
        "    \"img_size\": img_size,\n",
        "    \"patch_size\": patch_size,\n",
        "    \"n_channels\": n_channels,\n",
        "    \"n_heads\": n_heads,\n",
        "    \"n_layers\": n_layers,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs,\n",
        "    \"alpha\": alpha\n",
        "}\n",
        "\n",
        "wandb.config.update(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOATVnjpQd2R"
      },
      "source": [
        "# Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi2nvpZ4QfqH",
        "outputId": "85974779-ac62-4573-8324-d2c0070d096c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = T.Compose([\n",
        "  # T.Resize(img_size),\n",
        "  T.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = CIFAR10(\n",
        "  root=\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dataset\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_set = CIFAR10(\n",
        "  root=\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dataset\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3GesD8vQhcj"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vit_model import VisionTransformer\n",
        "\n",
        "\n",
        "def train_transformer(transformer,save_path, criterion, epochs, optimizer):\n",
        "   \n",
        "    # Setup\n",
        "    init_val_loss = np.inf\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device: \", device, \n",
        "          f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
        "    \n",
        "    # Ensure the model is on the proper device.\n",
        "    transformer.to(device)\n",
        "    \n",
        "    # Training & Validation loop\n",
        "    for epoch in range(epochs):\n",
        "        transformer.train()\n",
        "        training_loss = 0.0\n",
        "        \n",
        "        # Training loop\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = transformer(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.item()\n",
        "        \n",
        "        avg_loss = training_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs} - Train loss: {avg_loss:.3f}')\n",
        "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_loss})\n",
        "        \n",
        "        # Validation loop\n",
        "        transformer.eval()\n",
        "        validation_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for val_inputs, val_labels in test_loader:\n",
        "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "                val_outputs = transformer(val_inputs)\n",
        "                val_loss = criterion(val_outputs, val_labels)\n",
        "                validation_loss += val_loss.item()\n",
        "        \n",
        "        avg_val_loss = validation_loss / len(test_loader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs} - Validation loss: {avg_val_loss:.3f}')\n",
        "        wandb.log({\"validation_loss\": avg_val_loss})\n",
        "        \n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < init_val_loss:\n",
        "            init_val_loss = avg_val_loss\n",
        "            torch.save(transformer.state_dict(), os.path.join(save_path, \"best.pt\"))\n",
        "        \n",
        "        # Log a few sample predictions from the last validation batch.\n",
        "        sample_inputs = val_inputs[:4].detach().cpu()\n",
        "        sample_labels = val_labels[:4].detach().cpu()\n",
        "        sample_outputs = val_outputs[:4].detach().cpu()\n",
        "        _, sample_preds = torch.max(sample_outputs, 1)\n",
        "        \n",
        "        samples = []\n",
        "        for idx in range(len(sample_inputs)):\n",
        "            # Convert image from (C, H, W) to (H, W, C) for plotting.\n",
        "            image_np = sample_inputs[idx].permute(1, 2, 0).numpy()\n",
        "            plt.figure(figsize=(2,2))\n",
        "            plt.imshow(image_np)\n",
        "            plt.title(f\"GT: {sample_labels[idx].item()} | Pred: {sample_preds[idx].item()}\")\n",
        "            plt.axis(\"off\")\n",
        "            fig = plt.gcf()\n",
        "            samples.append(wandb.Image(fig))\n",
        "            plt.close(fig)\n",
        "        \n",
        "        wandb.log({\"sample_predictions\": samples, \"epoch\": epoch + 1})\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded config file: vit_config.yaml\n"
          ]
        }
      ],
      "source": [
        "def upload_config(config_path):\n",
        "    artifact_config = wandb.Artifact(\"config_file\", type=\"config\")\n",
        "    artifact_config.add_file(config_path)\n",
        "    wandb.log_artifact(artifact_config)\n",
        "    print(f\"Uploaded config file: {config_path}\")\n",
        "\n",
        "upload_config(\"vit_config.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device:  cuda (NVIDIA GeForce RTX 4060 Ti)\n",
            "Epoch 1/1 - Train loss: 2.225\n",
            "Epoch 1/1 - Validation loss: 2.140\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
        "\n",
        "save_path = exp_name\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = epochs\n",
        "optimizer = Adam(transformer.parameters(), lr=alpha)\n",
        "train_transformer(transformer = transformer,\n",
        "                    save_path=save_path, \n",
        "                    criterion=criterion, \n",
        "                    epochs=epochs, \n",
        "                    optimizer=optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKkxpNwyQi70",
        "outputId": "8f4468d2-132d-4b89-98d5-6caa16ff5aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device:  cuda (NVIDIA GeForce RTX 4060 Ti)\n",
            "Epoch 1/1 loss: 2.257\n",
            "Epoch 1/1 Validation loss: 2.220\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Setup \n",
        "# init_val_loss = np.inf\n",
        "# save_path = \"chkpt\"\n",
        "# os.makedirs(save_path, exist_ok=True)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
        "\n",
        "# transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
        "\n",
        "# optimizer = Adam(transformer.parameters(), lr=alpha)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     transformer.train()  \n",
        "#     training_loss = 0.0\n",
        "    \n",
        "#     for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = transformer(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         training_loss += loss.item()\n",
        "    \n",
        "#     avg_loss = training_loss / len(train_loader)\n",
        "#     print(f'Epoch {epoch + 1}/{epochs} loss: {avg_loss:.3f}')\n",
        "    \n",
        "#     wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_loss})\n",
        "    \n",
        "\n",
        "#     sample_inputs = inputs[:4].detach().cpu() \n",
        "#     sample_labels = labels[:4].detach().cpu()\n",
        "#     sample_outputs = outputs[:4].detach().cpu()\n",
        "#     _, sample_preds = torch.max(sample_outputs, 1)\n",
        "    \n",
        "    \n",
        "    \n",
        "#     #validation\n",
        "#     transformer.eval()  # Set model to evaluation mode\n",
        "#     validation_loss = 0.0\n",
        "#     with torch.no_grad():\n",
        "#         for val_inputs, val_labels in test_loader:\n",
        "#             val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "#             val_outputs = transformer(val_inputs)\n",
        "#             val_loss = criterion(val_outputs, val_labels)\n",
        "#             validation_loss += val_loss.item()\n",
        "    \n",
        "#     avg_val_loss = validation_loss / len(test_loader)\n",
        "#     if avg_val_loss < init_val_loss:\n",
        "#         init_val_loss = avg_val_loss\n",
        "#         torch.save(transformer.state_dict(), os.path.join(save_path,\"best.pt\"))\n",
        "#     print(f'Epoch {epoch + 1}/{epochs} Validation loss: {avg_val_loss:.3f}')\n",
        "#     wandb.log({\"validation_loss\": avg_val_loss})\n",
        "    \n",
        "#     # Optionally, log a few sample predictions from training or validation.\n",
        "#     # Here we log samples from the last val batch.\n",
        "#     sample_inputs = val_inputs[:4].detach().cpu()  # First 4 images from the last batch\n",
        "#     sample_labels = val_labels[:4].detach().cpu()\n",
        "#     sample_outputs = val_outputs[:4].detach().cpu()\n",
        "#     _, sample_preds = torch.max(sample_outputs, 1)\n",
        "    \n",
        "#     samples = []\n",
        "#     for idx in range(len(sample_inputs)):\n",
        "#         # Convert image from (C, H, W) to (H, W, C) for plotting if needed\n",
        "#         image_np = sample_inputs[idx].permute(1, 2, 0).numpy()\n",
        "#         plt.figure(figsize=(2,2))\n",
        "#         plt.imshow(image_np)\n",
        "#         plt.title(f\"GT: {sample_labels[idx].item()} | Pred: {sample_preds[idx].item()}\")\n",
        "#         plt.axis(\"off\")\n",
        "#         fig = plt.gcf()\n",
        "#         samples.append(wandb.Image(fig))\n",
        "#         plt.close(fig)\n",
        "    \n",
        "#     # wandb.log({\"sample_predictions\": samples, \"epoch\": epoch + 1})\n",
        "#     wandb.log({\"sample_predictions\": samples, \"epoch\": epoch + 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tRRL4gDQlFN"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwXO-XiEQnFy",
        "outputId": "31e591e9-da67-4807-d3f7-b82ecb265076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Accuracy: 43.98 %\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▇▆▆▆▅▄▄▄▃▃▄▂▃▃▃▃▂▂▃▃▂▂▂▁▁▁▂▁▂▂▂▂▁▁▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>test_accuracy</td><td>43.98</td></tr><tr><td>train_loss</td><td>2.00195</td></tr><tr><td>validation_loss</td><td>2.01837</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vit-patchsize-8-attention_head-3-layer-3</strong> at: <a href='https://wandb.ai/manna1/vit-image-classification/runs/sg76bjkj' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification/runs/sg76bjkj</a><br> View project at: <a href='https://wandb.ai/manna1/vit-image-classification' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification</a><br>Synced 5 W&B file(s), 401 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250415_103630-sg76bjkj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# correct = 0\n",
        "# total = 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   for data in test_loader:\n",
        "#     images, labels = data\n",
        "#     images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#     outputs = transformer(images)\n",
        "\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "#   print(f'\\nModel Accuracy: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "\n",
        "# Testing loop: accumulate predictions and ground truths, then log accuracy and confusion matrix.\n",
        "transformer.eval()  # Set to evaluation mode\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = transformer(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Accumulate predictions and labels for confusion matrix\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'\\nModel Accuracy: {accuracy:.2f} %')\n",
        "    wandb.log({\"test_accuracy\": accuracy})\n",
        "\n",
        "# Compute confusion matrix using sklearn\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "\n",
        "# Plot confusion matrix as a heatmap for logging.\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    \n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    \n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "# Define class names for the confusion matrix (modify as needed)\n",
        "class_names = [str(i) for i in range(n_classes)]\n",
        "cm_fig = plot_confusion_matrix(cm, classes=class_names, title=\"Confusion Matrix\")\n",
        "\n",
        "# Log the confusion matrix figure with wandb\n",
        "wandb.log({\"confusion_matrix\": wandb.Image(cm_fig)})\n",
        "\n",
        "# Optionally, you can finish the wandb run:\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7719494e37c0>> (for pre_run_cell):\n"
          ]
        },
        {
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:552\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:777\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 777\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attention statistics for image 1:\n",
            "Last layer attention shape: (10, 257, 257)\n",
            "Head 1: min=0.0033, max=0.0047, mean=0.0039\n",
            "Head 2: min=0.0018, max=0.0059, mean=0.0039\n",
            "Head 3: min=0.0025, max=0.0057, mean=0.0039\n",
            "Head 4: min=0.0032, max=0.0048, mean=0.0039\n",
            "Head 5: min=0.0028, max=0.0055, mean=0.0039\n",
            "Head 6: min=0.0023, max=0.0063, mean=0.0039\n",
            "Head 7: min=0.0033, max=0.0049, mean=0.0039\n",
            "Head 8: min=0.0028, max=0.0052, mean=0.0039\n",
            "Head 9: min=0.0028, max=0.0057, mean=0.0039\n",
            "Head 10: min=0.0017, max=0.0079, mean=0.0039\n",
            "Average attention: min=0.0034, max=0.0045, mean=0.0039\n",
            "Attention maps for image 1 saved to attention_maps/\n",
            "Saved files:\n",
            "- attention_maps/all_heads_last_layer_img1.png (32.3 KB)\n",
            "- attention_maps/avg_attention_last_layer_img1.png (11.6 KB)\n",
            "- attention_maps/attention_across_layers_img1.png (30.5 KB)\n",
            "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7719494e37c0>> (for post_run_cell):\n"
          ]
        },
        {
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:547\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:769\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:289\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/v10/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "\n",
        "# def visualize_attention(model, image, image_idx, save_dir='attention_maps', n_heads=3, n_layers= 2):\n",
        "#     # Create directory for saving attention maps\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     # Set model to evaluation mode\n",
        "#     model.train(False)\n",
        "\n",
        "#     # Forward pass to get attention weights\n",
        "#     _ = model(image)\n",
        "\n",
        "#     # Get number of patches in each dimension\n",
        "#     n_patches_h = img_size[0] // patch_size[0]\n",
        "#     n_patches_w = img_size[1] // patch_size[1]\n",
        "\n",
        "#     # Original image for reference\n",
        "#     img = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "#     img = (img * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
        "\n",
        "#     # Visualize attention from the last layer\n",
        "#     last_layer = model.transformer_encoder[-1]\n",
        "#     attn_weights = last_layer.mha.attn_weights.cpu().numpy()[0]  # [n_heads, seq_len, seq_len]\n",
        "\n",
        "#     # Plot attention from [CLS] token to patch tokens for each head in the last layer\n",
        "#     fig, axes = plt.subplots(1, n_heads, figsize=(15, 10))\n",
        "#     axes = axes.flatten()\n",
        "\n",
        "#     for h in range(n_heads):\n",
        "#         # Get attention weights from [CLS] token (index 0) to all patch tokens\n",
        "#         cls_attn = attn_weights[h, 0, 1:]  # Skip the [CLS] token itself\n",
        "\n",
        "#         # Reshape to match the image patches\n",
        "#         attn_map = cls_attn.reshape(n_patches_h, n_patches_w)\n",
        "#         # Plot\n",
        "#         axes[h].imshow(img)\n",
        "#         im = axes[h].imshow(attn_map, alpha=0.5, cmap='hot')\n",
        "#         axes[h].set_title(f'Head {h+1}')\n",
        "#         axes[h].axis('off')\n",
        "\n",
        "#     # Save the figure with all heads\n",
        "#     fig.suptitle(f'Attention Maps from [CLS] Token (Last Layer) - Image {image_idx}')\n",
        "#     fig.tight_layout()\n",
        "#     fig.savefig(f'{save_dir}/all_heads_last_layer_img{image_idx}.png')\n",
        "\n",
        "#     # Average attention across all heads\n",
        "#     avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
        "#     avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
        "\n",
        "#     # Plot average attention\n",
        "#     fig_avg = plt.figure(figsize=(8, 8))\n",
        "#     plt.imshow(img)\n",
        "#     plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
        "#     plt.title('Average Attention (Last Layer)')\n",
        "#     plt.axis('off')\n",
        "#     plt.savefig(f'{save_dir}/avg_attention_last_layer_img{image_idx}.png')\n",
        "#     plt.close(fig_avg)\n",
        "\n",
        "#     # Visualize attention across all layers\n",
        "#     fig_layers = plt.figure(figsize=(15, 10))\n",
        "\n",
        "#     for l, encoder in enumerate(model.transformer_encoder):\n",
        "#         attn_weights = encoder.mha.attn_weights.cpu().numpy()[0]\n",
        "\n",
        "#         # Average attention across all heads for this layer\n",
        "#         avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
        "#         avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
        "\n",
        "#         # Plot\n",
        "#         plt.subplot(1, n_layers, l+1)\n",
        "#         plt.imshow(img)\n",
        "#         plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
        "#         plt.title(f'Layer {l+1}')\n",
        "#         plt.axis('off')\n",
        "\n",
        "#     plt.suptitle(f'Attention Maps Across Layers - Image {image_idx}')\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(f'{save_dir}/attention_across_layers_img{image_idx}.png')\n",
        "#     plt.close(fig_layers)\n",
        "\n",
        "#     # Close the first figure\n",
        "#     plt.close(fig)\n",
        "\n",
        "#     # Print statistics about the attention maps for verification\n",
        "#     print(f\"\\nAttention statistics for image {image_idx}:\")\n",
        "#     print(f\"Last layer attention shape: {attn_weights.shape}\")\n",
        "\n",
        "#     # Print min, max, mean values for each head in the last layer\n",
        "#     for h in range(n_heads):\n",
        "#         head_attn = attn_weights[h, 0, 1:]\n",
        "#         print(f\"Head {h+1}: min={head_attn.min():.4f}, max={head_attn.max():.4f}, mean={head_attn.mean():.4f}\")\n",
        "\n",
        "#     # Print average attention statistics\n",
        "#     print(f\"Average attention: min={avg_attn.min():.4f}, max={avg_attn.max():.4f}, mean={avg_attn.mean():.4f}\")\n",
        "\n",
        "#     print(f\"Attention maps for image {image_idx} saved to {save_dir}/\")\n",
        "\n",
        "#     # List the saved files\n",
        "#     saved_files = [\n",
        "#         f'{save_dir}/all_heads_last_layer_img{image_idx}.png',\n",
        "#         f'{save_dir}/avg_attention_last_layer_img{image_idx}.png',\n",
        "#         f'{save_dir}/attention_across_layers_img{image_idx}.png'\n",
        "#     ]\n",
        "\n",
        "#     print(\"Saved files:\")\n",
        "#     for file in saved_files:\n",
        "#         if os.path.exists(file):\n",
        "#             print(f\"- {file} ({os.path.getsize(file) / 1024:.1f} KB)\")\n",
        "#         else:\n",
        "#             print(f\"- {file} (File not created)\")\n",
        "\n",
        "#     return saved_files\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     # model = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers).to(device)\n",
        "#     for i, data in enumerate(test_loader, 0):\n",
        "#             inputs, labels = data\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             break\n",
        "#     visualize_attention(transformer,inputs[0].unsqueeze(0), 1, n_layers = n_layers, n_heads = n_heads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Using device: cuda:0\n",
            "[1, 50] loss: 2.302\n",
            "[1, 100] loss: 2.270\n",
            "Finished Training\n",
            "\n",
            "Attention statistics for image 1:\n",
            "Last layer attention shape: (6, 257, 257)\n",
            "Head 1: min=0.0024, max=0.0083, mean=0.0039\n",
            "Head 2: min=0.0023, max=0.0050, mean=0.0039\n",
            "Head 3: min=0.0011, max=0.0093, mean=0.0039\n",
            "Head 4: min=0.0033, max=0.0049, mean=0.0039\n",
            "Head 5: min=0.0031, max=0.0049, mean=0.0039\n",
            "Head 6: min=0.0035, max=0.0046, mean=0.0039\n",
            "Average attention: min=0.0034, max=0.0047, mean=0.0039\n",
            "Attention maps for image 1 saved to attention_maps/\n",
            "Saved files:\n",
            "- attention_maps/all_heads_last_layer_img1.png (28.5 KB)\n",
            "- attention_maps/avg_attention_last_layer_img1.png (11.5 KB)\n",
            "- attention_maps/attention_across_layers_img1.png (27.0 KB)\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib\n",
        "# matplotlib.use('Agg')  # Use non-interactive backend\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# from torch.utils.data import DataLoader\n",
        "# import os\n",
        "\n",
        "# # Define the components needed for the Vision Transformer\n",
        "# class PatchEmbedding(nn.Module):\n",
        "#     def __init__(self, d_model, img_size, patch_size, n_channels):\n",
        "#         super().__init__()\n",
        "#         self.d_model = d_model\n",
        "#         self.img_size = img_size\n",
        "#         self.patch_size = patch_size\n",
        "#         self.n_channels = n_channels\n",
        "\n",
        "#         # Linear projection\n",
        "#         self.proj = nn.Conv2d(n_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "#         # Class token\n",
        "#         self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.shape\n",
        "\n",
        "#         # Project patches\n",
        "#         x = self.proj(x)  # (B, d_model, H/patch_size, W/patch_size)\n",
        "#         x = x.flatten(2)  # (B, d_model, n_patches)\n",
        "#         x = x.transpose(1, 2)  # (B, n_patches, d_model)\n",
        "\n",
        "#         # Add class token\n",
        "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "#         x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, max_seq_length):\n",
        "#         super().__init__()\n",
        "#         self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_length, d_model))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return x + self.pos_embedding\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, d_model, n_heads):\n",
        "#         super().__init__()\n",
        "#         self.d_model = d_model\n",
        "#         self.n_heads = n_heads\n",
        "#         self.head_dim = d_model // n_heads\n",
        "\n",
        "#         self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "#         self.proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "#         self.attn_weights = None  # Store attention weights for visualization\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, N, C = x.shape\n",
        "\n",
        "#         qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "#         attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "#         attn = attn.softmax(dim=-1)\n",
        "\n",
        "#         # Save attention weights for visualization\n",
        "#         self.attn_weights = attn.detach().clone()\n",
        "\n",
        "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "#         x = self.proj(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# class TransformerEncoder(nn.Module):\n",
        "#     def __init__(self, d_model, n_heads):\n",
        "#         super().__init__()\n",
        "#         self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "#         self.norm1 = nn.LayerNorm(d_model)\n",
        "#         self.norm2 = nn.LayerNorm(d_model)\n",
        "#         self.mlp = nn.Sequential(\n",
        "#             nn.Linear(d_model, 4 * d_model),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(4 * d_model, d_model)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.attn(self.norm1(x))\n",
        "#         x = x + self.mlp(self.norm2(x))\n",
        "#         return x\n",
        "\n",
        "# class VisionTransformer(nn.Module):\n",
        "#     def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
        "#         super().__init__()\n",
        "\n",
        "#         assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
        "#         assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "#         self.d_model = d_model # Dimensionality of model\n",
        "#         self.n_classes = n_classes # Number of classes\n",
        "#         self.img_size = img_size # Image size\n",
        "#         self.patch_size = patch_size # Patch size\n",
        "#         self.n_channels = n_channels # Number of channels\n",
        "#         self.n_heads = n_heads # Number of attention heads\n",
        "\n",
        "#         self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
        "#         self.max_seq_length = self.n_patches + 1\n",
        "\n",
        "#         self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
        "#         self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
        "\n",
        "#         # Use ModuleList instead of Sequential to access individual layers\n",
        "#         self.transformer_encoders = nn.ModuleList([TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)])\n",
        "\n",
        "#         # Classification MLP\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(self.d_model, self.n_classes),\n",
        "#             nn.Softmax(dim=-1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, images):\n",
        "#         x = self.patch_embedding(images)\n",
        "#         x = self.positional_encoding(x)\n",
        "\n",
        "#         # Pass through each transformer encoder layer\n",
        "#         for encoder in self.transformer_encoders:\n",
        "#             x = encoder(x)\n",
        "\n",
        "#         # Use the [CLS] token for classification\n",
        "#         x = self.classifier(x[:,0])\n",
        "\n",
        "#         return x\n",
        "\n",
        "# # Function to train a simple ViT model on CIFAR-10\n",
        "# def train_vit_model(model, epochs=1):\n",
        "#     # Load CIFAR-10 training dataset\n",
        "#     transform_train = transforms.Compose([\n",
        "#         transforms.RandomCrop(32, padding=4),\n",
        "#         transforms.RandomHorizontalFlip(),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "#     ])\n",
        "\n",
        "#     trainset = torchvision.datasets.CIFAR10(root='./dataset', train=True, download=True, transform=transform_train)\n",
        "#     trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "#     # Define loss function and optimizer\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#     # Training loop\n",
        "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(f\"Using device: {device}\")\n",
        "#     model.to(device)\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         running_loss = 0.0\n",
        "#         for i, data in enumerate(trainloader, 0):\n",
        "#             inputs, labels = data\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "#             if i % 50 == 49:\n",
        "#                 print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 50:.3f}')\n",
        "#                 running_loss = 0.0\n",
        "#                 # Break early for demonstration purposes\n",
        "#                 if i >= 99:\n",
        "#                     break\n",
        "\n",
        "#     print('Finished Training')\n",
        "#     return model\n",
        "\n",
        "# # Function to visualize attention maps\n",
        "# def visualize_attention(model, image, image_idx, save_dir='attention_maps'):\n",
        "#     # Create directory for saving attention maps\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     # Set model to evaluation mode\n",
        "#     model.train(False)\n",
        "\n",
        "#     # Forward pass to get attention weights\n",
        "#     _ = model(image)\n",
        "\n",
        "#     # Get number of patches in each dimension\n",
        "#     n_patches_h = img_size[0] // patch_size[0]\n",
        "#     n_patches_w = img_size[1] // patch_size[1]\n",
        "\n",
        "#     # Original image for reference\n",
        "#     img = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "#     img = (img * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
        "\n",
        "#     # Visualize attention from the last layer\n",
        "#     last_layer = model.transformer_encoders[-1]\n",
        "#     attn_weights = last_layer.attn.attn_weights.cpu().numpy()[0]  # [n_heads, seq_len, seq_len]\n",
        "\n",
        "#     # Plot attention from [CLS] token to patch tokens for each head in the last layer\n",
        "#     fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "#     axes = axes.flatten()\n",
        "\n",
        "#     for h in range(n_heads):\n",
        "#         # Get attention weights from [CLS] token (index 0) to all patch tokens\n",
        "#         cls_attn = attn_weights[h, 0, 1:]  # Skip the [CLS] token itself\n",
        "\n",
        "#         # Reshape to match the image patches\n",
        "#         attn_map = cls_attn.reshape(n_patches_h, n_patches_w)\n",
        "#         # Plot\n",
        "#         axes[h].imshow(img)\n",
        "#         im = axes[h].imshow(attn_map, alpha=0.5, cmap='hot')\n",
        "#         axes[h].set_title(f'Head {h+1}')\n",
        "#         axes[h].axis('off')\n",
        "\n",
        "#     # Save the figure with all heads\n",
        "#     fig.suptitle(f'Attention Maps from [CLS] Token (Last Layer) - Image {image_idx}')\n",
        "#     fig.tight_layout()\n",
        "#     fig.savefig(f'{save_dir}/all_heads_last_layer_img{image_idx}.png')\n",
        "\n",
        "#     # Average attention across all heads\n",
        "#     avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
        "#     avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
        "\n",
        "#     # Plot average attention\n",
        "#     fig_avg = plt.figure(figsize=(8, 8))\n",
        "#     plt.imshow(img)\n",
        "#     plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
        "#     plt.title('Average Attention (Last Layer)')\n",
        "#     plt.axis('off')\n",
        "#     plt.savefig(f'{save_dir}/avg_attention_last_layer_img{image_idx}.png')\n",
        "#     plt.close(fig_avg)\n",
        "\n",
        "#     # Visualize attention across all layers\n",
        "#     fig_layers = plt.figure(figsize=(15, 10))\n",
        "\n",
        "#     for l, encoder in enumerate(model.transformer_encoders):\n",
        "#         attn_weights = encoder.attn.attn_weights.cpu().numpy()[0]\n",
        "\n",
        "#         # Average attention across all heads for this layer\n",
        "#         avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
        "#         avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
        "\n",
        "#         # Plot\n",
        "#         plt.subplot(2, 3, l+1)\n",
        "#         plt.imshow(img)\n",
        "#         plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
        "#         plt.title(f'Layer {l+1}')\n",
        "#         plt.axis('off')\n",
        "\n",
        "#     plt.suptitle(f'Attention Maps Across Layers - Image {image_idx}')\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(f'{save_dir}/attention_across_layers_img{image_idx}.png')\n",
        "#     plt.close(fig_layers)\n",
        "\n",
        "#     # Close the first figure\n",
        "#     plt.close(fig)\n",
        "\n",
        "#     # Print statistics about the attention maps for verification\n",
        "#     print(f\"\\nAttention statistics for image {image_idx}:\")\n",
        "#     print(f\"Last layer attention shape: {attn_weights.shape}\")\n",
        "\n",
        "#     # Print min, max, mean values for each head in the last layer\n",
        "#     for h in range(n_heads):\n",
        "#         head_attn = attn_weights[h, 0, 1:]\n",
        "#         print(f\"Head {h+1}: min={head_attn.min():.4f}, max={head_attn.max():.4f}, mean={head_attn.mean():.4f}\")\n",
        "\n",
        "#     # Print average attention statistics\n",
        "#     print(f\"Average attention: min={avg_attn.min():.4f}, max={avg_attn.max():.4f}, mean={avg_attn.mean():.4f}\")\n",
        "\n",
        "#     print(f\"Attention maps for image {image_idx} saved to {save_dir}/\")\n",
        "\n",
        "#     # List the saved files\n",
        "#     saved_files = [\n",
        "#         f'{save_dir}/all_heads_last_layer_img{image_idx}.png',\n",
        "#         f'{save_dir}/avg_attention_last_layer_img{image_idx}.png',\n",
        "#         f'{save_dir}/attention_across_layers_img{image_idx}.png'\n",
        "#     ]\n",
        "\n",
        "#     print(\"Saved files:\")\n",
        "#     for file in saved_files:\n",
        "#         if os.path.exists(file):\n",
        "#             print(f\"- {file} ({os.path.getsize(file) / 1024:.1f} KB)\")\n",
        "#         else:\n",
        "#             print(f\"- {file} (File not created)\")\n",
        "\n",
        "#     return saved_files\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     d_model = 12\n",
        "#     n_classes = 10\n",
        "#     img_size = (32,32)\n",
        "#     patch_size = (2,2)\n",
        "#     n_channels = 3\n",
        "#     n_heads = 6\n",
        "#     n_layers = 6\n",
        "#     batch_size = 128\n",
        "#     epochs = 100\n",
        "#     alpha = 0.005\n",
        "#     model = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers)\n",
        "#     train_vit_model(model)\n",
        "#     for i, data in enumerate(test_loader, 0):\n",
        "#             inputs, labels = data\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             break\n",
        "#     visualize_attention(model,inputs[0].unsqueeze(0), 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib\n",
        "# matplotlib.use('Agg')  # Use non-interactive backend\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# from torch.utils.data import DataLoader\n",
        "# import os\n",
        "# from skimage.measure import find_contours\n",
        "# from matplotlib.patches import Polygon\n",
        "# import cv2\n",
        "# import colorsys\n",
        "# import random\n",
        "\n",
        "# # # Define the components needed for the Vision Transformer\n",
        "# # class PatchEmbedding(nn.Module):\n",
        "# #     def __init__(self, d_model, img_size, patch_size, n_channels):\n",
        "# #         super().__init__()\n",
        "# #         self.d_model = d_model\n",
        "# #         self.img_size = img_size\n",
        "# #         self.patch_size = patch_size\n",
        "# #         self.n_channels = n_channels\n",
        "\n",
        "# #         # Linear projection\n",
        "# #         self.proj = nn.Conv2d(n_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "# #         # Class token\n",
        "# #         self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "# #     def forward(self, x):\n",
        "# #         B, C, H, W = x.shape\n",
        "\n",
        "# #         # Project patches\n",
        "# #         x = self.proj(x)  # (B, d_model, H/patch_size, W/patch_size)\n",
        "# #         x = x.flatten(2)  # (B, d_model, n_patches)\n",
        "# #         x = x.transpose(1, 2)  # (B, n_patches, d_model)\n",
        "\n",
        "# #         # Add class token\n",
        "# #         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "# #         x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "# #         return x\n",
        "\n",
        "# # class PositionalEncoding(nn.Module):\n",
        "# #     def __init__(self, d_model, max_seq_length):\n",
        "# #         super().__init__()\n",
        "# #         self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_length, d_model))\n",
        "\n",
        "# #     def forward(self, x):\n",
        "# #         return x + self.pos_embedding\n",
        "\n",
        "# # class MultiHeadAttention(nn.Module):\n",
        "# #     def __init__(self, d_model, n_heads):\n",
        "# #         super().__init__()\n",
        "# #         self.d_model = d_model\n",
        "# #         self.n_heads = n_heads\n",
        "# #         self.head_dim = d_model // n_heads\n",
        "\n",
        "# #         self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "# #         self.proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "# #         self.attn_weights = None  # Store attention weights for visualization\n",
        "\n",
        "# #     def forward(self, x):\n",
        "# #         B, N, C = x.shape\n",
        "\n",
        "# #         qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "# #         q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "# #         attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "# #         attn = attn.softmax(dim=-1)\n",
        "\n",
        "# #         # Save attention weights for visualization\n",
        "# #         self.attn_weights = attn.detach().clone()\n",
        "\n",
        "# #         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "# #         x = self.proj(x)\n",
        "\n",
        "# #         return x\n",
        "\n",
        "# # class TransformerEncoder(nn.Module):\n",
        "# #     def __init__(self, d_model, n_heads):\n",
        "# #         super().__init__()\n",
        "# #         self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "# #         self.norm1 = nn.LayerNorm(d_model)\n",
        "# #         self.norm2 = nn.LayerNorm(d_model)\n",
        "# #         self.mlp = nn.Sequential(\n",
        "# #             nn.Linear(d_model, 4 * d_model),\n",
        "# #             nn.GELU(),\n",
        "# #             nn.Linear(4 * d_model, d_model)\n",
        "# #         )\n",
        "\n",
        "# #     def forward(self, x):\n",
        "# #         x = x + self.attn(self.norm1(x))\n",
        "# #         x = x + self.mlp(self.norm2(x))\n",
        "# #         return x\n",
        "\n",
        "# # class VisionTransformer(nn.Module):\n",
        "# #     def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
        "# #         super().__init__()\n",
        "\n",
        "# #         assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
        "# #         assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "# #         self.d_model = d_model # Dimensionality of model\n",
        "# #         self.n_classes = n_classes # Number of classes\n",
        "# #         self.img_size = img_size # Image size\n",
        "# #         self.patch_size = patch_size # Patch size\n",
        "# #         self.n_channels = n_channels # Number of channels\n",
        "# #         self.n_heads = n_heads # Number of attention heads\n",
        "\n",
        "# #         self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
        "# #         self.max_seq_length = self.n_patches + 1\n",
        "\n",
        "# #         self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
        "# #         self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
        "\n",
        "# #         # Use ModuleList instead of Sequential to access individual layers\n",
        "# #         self.transformer_encoders = nn.ModuleList([TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)])\n",
        "\n",
        "# #         # Classification MLP\n",
        "# #         self.classifier = nn.Sequential(\n",
        "# #             nn.Linear(self.d_model, self.n_classes),\n",
        "# #             nn.Softmax(dim=-1)\n",
        "# #         )\n",
        "\n",
        "# #     def forward(self, images):\n",
        "# #         x = self.patch_embedding(images)\n",
        "# #         x = self.positional_encoding(x)\n",
        "\n",
        "# #         # Pass through each transformer encoder layer\n",
        "# #         for encoder in self.transformer_encoders:\n",
        "# #             x = encoder(x)\n",
        "\n",
        "# #         # Use the [CLS] token for classification\n",
        "# #         x = self.classifier(x[:,0])\n",
        "\n",
        "# #         return x\n",
        "\n",
        "\n",
        "# #########################################\n",
        "# # Define the components needed for the Vision Transformer\n",
        "# class PatchEmbedding(nn.Module):\n",
        "#     def __init__(self, d_model, img_size, patch_size, n_channels):\n",
        "#         super().__init__()\n",
        "#         self.d_model = d_model\n",
        "#         self.img_size = img_size\n",
        "#         self.patch_size = patch_size\n",
        "#         self.n_channels = n_channels\n",
        "\n",
        "#         # Linear projection\n",
        "#         self.proj = nn.Conv2d(n_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "#         # Class token\n",
        "#         self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.shape\n",
        "\n",
        "#         # Project patches\n",
        "#         x = self.proj(x)  # (B, d_model, H/patch_size, W/patch_size)\n",
        "#         x = x.flatten(2)  # (B, d_model, n_patches)\n",
        "#         x = x.transpose(1, 2)  # (B, n_patches, d_model)\n",
        "\n",
        "#         # Add class token\n",
        "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "#         x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, d_model, max_seq_length):\n",
        "#         super().__init__()\n",
        "#         self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_length, d_model))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return x + self.pos_embedding\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, d_model, n_heads):\n",
        "#         super().__init__()\n",
        "#         self.d_model = d_model\n",
        "#         self.n_heads = n_heads\n",
        "#         self.head_dim = d_model // n_heads\n",
        "\n",
        "#         self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "#         self.proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "#         self.attn_weights = None  # Store attention weights for visualization\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, N, C = x.shape\n",
        "\n",
        "#         qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "#         attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "#         attn = attn.softmax(dim=-1)\n",
        "\n",
        "#         # Save attention weights for visualization\n",
        "#         self.attn_weights = attn.detach().clone()\n",
        "\n",
        "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "#         x = self.proj(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# class TransformerEncoder(nn.Module):\n",
        "#     def __init__(self, d_model, n_heads):\n",
        "#         super().__init__()\n",
        "#         self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "#         self.norm1 = nn.LayerNorm(d_model)\n",
        "#         self.norm2 = nn.LayerNorm(d_model)\n",
        "#         self.mlp = nn.Sequential(\n",
        "#             nn.Linear(d_model, 4 * d_model),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(4 * d_model, d_model)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.attn(self.norm1(x))\n",
        "#         x = x + self.mlp(self.norm2(x))\n",
        "#         return x\n",
        "\n",
        "# class VisionTransformer(nn.Module):\n",
        "#     def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers):\n",
        "#         super().__init__()\n",
        "\n",
        "#         assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
        "#         assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "#         self.d_model = d_model # Dimensionality of model\n",
        "#         self.n_classes = n_classes # Number of classes\n",
        "#         self.img_size = img_size # Image size\n",
        "#         self.patch_size = patch_size # Patch size\n",
        "#         self.n_channels = n_channels # Number of channels\n",
        "#         self.n_heads = n_heads # Number of attention heads\n",
        "\n",
        "#         self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
        "#         self.max_seq_length = self.n_patches + 1\n",
        "\n",
        "#         self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
        "#         self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
        "\n",
        "#         # Use ModuleList instead of Sequential to access individual layers\n",
        "#         self.transformer_encoders = nn.ModuleList([TransformerEncoder(self.d_model, self.n_heads) for _ in range(n_layers)])\n",
        "\n",
        "#         # Classification MLP\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(self.d_model, self.n_classes),\n",
        "#             nn.Softmax(dim=-1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, images):\n",
        "#         x = self.patch_embedding(images)\n",
        "#         x = self.positional_encoding(x)\n",
        "\n",
        "#         # Pass through each transformer encoder layer\n",
        "#         for encoder in self.transformer_encoders:\n",
        "#             x = encoder(x)\n",
        "\n",
        "#         # Use the [CLS] token for classification\n",
        "#         x = self.classifier(x[:,0])\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def get_last_selfattention(self, x):\n",
        "#         x = self.patch_embedding(x)\n",
        "#         x = self.positional_encoding(x)\n",
        "\n",
        "#         # Pass through each transformer encoder layer except the last one\n",
        "#         for i, encoder in enumerate(self.transformer_encoders[:-1]):\n",
        "#             x = encoder(x)\n",
        "\n",
        "#         # For the last layer, we need to get the attention weights\n",
        "#         last_encoder = self.transformer_encoders[-1]\n",
        "#         x = last_encoder.norm1(x)\n",
        "\n",
        "#         # Extract QKV\n",
        "#         B, N, C = x.shape\n",
        "#         qkv = last_encoder.attn.qkv(x).reshape(B, N, 3, last_encoder.attn.n_heads, last_encoder.attn.head_dim).permute(2, 0, 3, 1, 4)\n",
        "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "#         # Calculate attention\n",
        "#         attn = (q @ k.transpose(-2, -1)) * (last_encoder.attn.head_dim ** -0.5)\n",
        "#         attn = attn.softmax(dim=-1)\n",
        "\n",
        "#         return attn\n",
        "\n",
        "# # Function to train a simple ViT model on CIFAR-10\n",
        "# def train_vit_model(model, epochs=1):\n",
        "#     # Load CIFAR-10 training dataset\n",
        "#     transform_train = transforms.Compose([\n",
        "#         transforms.RandomCrop(32, padding=4),\n",
        "#         transforms.RandomHorizontalFlip(),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "#     ])\n",
        "\n",
        "#     trainset = torchvision.datasets.CIFAR10(root='./dataset', train=True, download=True, transform=transform_train)\n",
        "#     trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "#     # Define loss function and optimizer\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#     # Training loop\n",
        "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     print(f\"Using device: {device}\")\n",
        "#     model.to(device)\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         running_loss = 0.0\n",
        "#         for i, data in enumerate(trainloader, 0):\n",
        "#             inputs, labels = data\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "#             if i % 50 == 49:\n",
        "#                 print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 50:.3f}')\n",
        "#                 running_loss = 0.0\n",
        "#                 # Break early for demonstration purposes\n",
        "#                 if i >= 99:\n",
        "#                     break\n",
        "\n",
        "#     print('Finished Training')\n",
        "#     return model\n",
        "\n",
        "# # Function to visualize attention maps\n",
        "# # def visualize_attention(model, image, image_idx, save_dir='attention_maps'):\n",
        "# #     # Create directory for saving attention maps\n",
        "# #     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# #     # Set model to evaluation mode\n",
        "# #     model.train(False)\n",
        "\n",
        "# #     # Forward pass to get attention weights\n",
        "# #     _ = model(image)\n",
        "\n",
        "# #     # Get number of patches in each dimension\n",
        "# #     n_patches_h = img_size[0] // patch_size[0]\n",
        "# #     n_patches_w = img_size[1] // patch_size[1]\n",
        "\n",
        "# #     # Original image for reference\n",
        "# #     img = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "# #     img = (img * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
        "\n",
        "# #     # Visualize attention from the last layer\n",
        "# #     last_layer = model.transformer_encoders[-1]\n",
        "# #     attn_weights = last_layer.attn.attn_weights.cpu().numpy()[0]  # [n_heads, seq_len, seq_len]\n",
        "\n",
        "# #     # Plot attention from [CLS] token to patch tokens for each head in the last layer\n",
        "# #     fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "# #     axes = axes.flatten()\n",
        "\n",
        "# #     for h in range(n_heads):\n",
        "# #         # Get attention weights from [CLS] token (index 0) to all patch tokens\n",
        "# #         cls_attn = attn_weights[h, 0, 1:]  # Skip the [CLS] token itself\n",
        "\n",
        "# #         # Reshape to match the image patches\n",
        "# #         attn_map = cls_attn.reshape(n_patches_h, n_patches_w)\n",
        "# #         # Plot\n",
        "# #         axes[h].imshow(img)\n",
        "# #         im = axes[h].imshow(attn_map, alpha=0.5, cmap='hot')\n",
        "# #         axes[h].set_title(f'Head {h+1}')\n",
        "# #         axes[h].axis('off')\n",
        "\n",
        "# #     # Save the figure with all heads\n",
        "# #     fig.suptitle(f'Attention Maps from [CLS] Token (Last Layer) - Image {image_idx}')\n",
        "# #     fig.tight_layout()\n",
        "# #     fig.savefig(f'{save_dir}/all_heads_last_layer_img{image_idx}.png')\n",
        "\n",
        "# #     # Average attention across all heads\n",
        "# #     avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
        "# #     avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
        "\n",
        "# #     # Plot average attention\n",
        "# #     fig_avg = plt.figure(figsize=(8, 8))\n",
        "# #     plt.imshow(img)\n",
        "# #     plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
        "# #     plt.title('Average Attention (Last Layer)')\n",
        "# #     plt.axis('off')\n",
        "# #     plt.savefig(f'{save_dir}/avg_attention_last_layer_img{image_idx}.png')\n",
        "# #     plt.close(fig_avg)\n",
        "\n",
        "# #     # Visualize attention across all layers\n",
        "# #     fig_layers = plt.figure(figsize=(15, 10))\n",
        "\n",
        "# #     for l, encoder in enumerate(model.transformer_encoders):\n",
        "# #         attn_weights = encoder.attn.attn_weights.cpu().numpy()[0]\n",
        "\n",
        "# #         # Average attention across all heads for this layer\n",
        "# #         avg_attn = attn_weights.mean(axis=0)[0, 1:]\n",
        "# #         avg_attn_map = avg_attn.reshape(n_patches_h, n_patches_w)\n",
        "\n",
        "# #         # Plot\n",
        "# #         plt.subplot(2, 3, l+1)\n",
        "# #         plt.imshow(img)\n",
        "# #         plt.imshow(avg_attn_map, alpha=0.5, cmap='hot')\n",
        "# #         plt.title(f'Layer {l+1}')\n",
        "# #         plt.axis('off')\n",
        "\n",
        "# #     plt.suptitle(f'Attention Maps Across Layers - Image {image_idx}')\n",
        "# #     plt.tight_layout()\n",
        "# #     plt.savefig(f'{save_dir}/attention_across_layers_img{image_idx}.png')\n",
        "# #     plt.close(fig_layers)\n",
        "\n",
        "# #     # Close the first figure\n",
        "# #     plt.close(fig)\n",
        "\n",
        "# #     # Print statistics about the attention maps for verification\n",
        "# #     print(f\"\\nAttention statistics for image {image_idx}:\")\n",
        "# #     print(f\"Last layer attention shape: {attn_weights.shape}\")\n",
        "\n",
        "# #     # Print min, max, mean values for each head in the last layer\n",
        "# #     for h in range(n_heads):\n",
        "# #         head_attn = attn_weights[h, 0, 1:]\n",
        "# #         print(f\"Head {h+1}: min={head_attn.min():.4f}, max={head_attn.max():.4f}, mean={head_attn.mean():.4f}\")\n",
        "\n",
        "# #     # Print average attention statistics\n",
        "# #     print(f\"Average attention: min={avg_attn.min():.4f}, max={avg_attn.max():.4f}, mean={avg_attn.mean():.4f}\")\n",
        "\n",
        "# #     print(f\"Attention maps for image {image_idx} saved to {save_dir}/\")\n",
        "\n",
        "# #     # List the saved files\n",
        "# #     saved_files = [\n",
        "# #         f'{save_dir}/all_heads_last_layer_img{image_idx}.png',\n",
        "# #         f'{save_dir}/avg_attention_last_layer_img{image_idx}.png',\n",
        "# #         f'{save_dir}/attention_across_layers_img{image_idx}.png'\n",
        "# #     ]\n",
        "\n",
        "# #     print(\"Saved files:\")\n",
        "# #     for file in saved_files:\n",
        "# #         if os.path.exists(file):\n",
        "# #             print(f\"- {file} ({os.path.getsize(file) / 1024:.1f} KB)\")\n",
        "# #         else:\n",
        "# #             print(f\"- {file} (File not created)\")\n",
        "\n",
        "# #     return saved_files\n",
        "\n",
        "# #######################\n",
        "# # Helper functions for visualization\n",
        "# def apply_mask(image, mask, color, alpha=0.5):\n",
        "#     \"\"\"Apply mask to image with specified color and transparency.\"\"\"\n",
        "#     for c in range(3):\n",
        "#         image[:, :, c] = image[:, :, c] * (1 - alpha * mask) + alpha * mask * color[c] * 255\n",
        "#     return image\n",
        "\n",
        "# def random_colors(N, bright=True):\n",
        "#     \"\"\"Generate random colors.\"\"\"\n",
        "#     brightness = 1.0 if bright else 0.7\n",
        "#     hsv = [(i / N, 1, brightness) for i in range(N)]\n",
        "#     colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
        "#     random.shuffle(colors)\n",
        "#     return colors\n",
        "\n",
        "# def display_instances(image, mask, fname=\"test\", figsize=(5, 5), blur=False, contour=True, alpha=0.5):\n",
        "#     \"\"\"Display segmentation masks on an image.\"\"\"\n",
        "#     fig = plt.figure(figsize=figsize, frameon=False)\n",
        "#     ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
        "#     ax.set_axis_off()\n",
        "#     fig.add_axes(ax)\n",
        "#     ax = plt.gca()\n",
        "\n",
        "#     N = 1\n",
        "#     mask = mask[None, :, :]\n",
        "#     # Generate random colors\n",
        "#     colors = random_colors(N)\n",
        "\n",
        "#     # Show area outside image boundaries.\n",
        "#     height, width = image.shape[:2]\n",
        "#     margin = 0\n",
        "#     ax.set_ylim(height + margin, -margin)\n",
        "#     ax.set_xlim(-margin, width + margin)\n",
        "#     ax.axis('off')\n",
        "#     masked_image = image.astype(np.uint32).copy()\n",
        "#     for i in range(N):\n",
        "#         color = colors[i]\n",
        "#         _mask = mask[i]\n",
        "#         if blur:\n",
        "#             _mask = cv2.blur(_mask, (10, 10))\n",
        "#         # Mask\n",
        "#         masked_image = apply_mask(masked_image, _mask, color, alpha)\n",
        "#         # Mask Polygon\n",
        "#         # Pad to ensure proper polygons for masks that touch image edges.\n",
        "#         if contour:\n",
        "#             padded_mask = np.zeros((_mask.shape[0] + 2, _mask.shape[1] + 2))\n",
        "#             padded_mask[1:-1, 1:-1] = _mask\n",
        "#             contours = find_contours(padded_mask, 0.5)\n",
        "#             for verts in contours:\n",
        "#                 # Subtract the padding and flip (y, x) to (x, y)\n",
        "#                 verts = np.fliplr(verts) - 1\n",
        "#                 p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
        "#                 ax.add_patch(p)\n",
        "#     ax.imshow(masked_image.astype(np.uint8), aspect='auto')\n",
        "#     fig.savefig(fname)\n",
        "#     plt.close(fig)\n",
        "#     print(f\"{fname} saved.\")\n",
        "#     return\n",
        "\n",
        "# def visualize_attention_improved(model, image, image_idx, threshold=0.6, save_dir='attention_maps_improved'):\n",
        "#     \"\"\"\n",
        "#     Visualize attention maps with improved visualization techniques.\n",
        "\n",
        "#     Args:\n",
        "#         model: The Vision Transformer model\n",
        "#         image: Input image tensor\n",
        "#         image_idx: Image index for saving files\n",
        "#         threshold: Threshold for attention mask (keep top % of attention mass)\n",
        "#         save_dir: Directory to save visualizations\n",
        "#     \"\"\"\n",
        "#     # Create directory for saving attention maps\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     # Set model to evaluation mode\n",
        "#     model.train(False)\n",
        "\n",
        "#     # Get attention weights from the last layer\n",
        "#     # last_layer = model.transformer_encoders[-1]\n",
        "#     # attentions = last_layer.attn.attn_weights.cpu().numpy()[0]  # [n_heads, seq_len, seq_len]\n",
        "\n",
        "#     attentions = model.get_last_selfattention(image)\n",
        "\n",
        "#     # Convert image for visualization\n",
        "#     img_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "#     img_np = (img_np * 0.5 + 0.5).clip(0, 1)  # Denormalize\n",
        "#     img_uint8 = (img_np * 255).astype(np.uint8)\n",
        "\n",
        "#     # Save the original image\n",
        "#     plt.figure(figsize=(5, 5))\n",
        "#     plt.imshow(img_np)\n",
        "#     plt.axis('off')\n",
        "#     plt.savefig(f'{save_dir}/original_img{image_idx}.png')\n",
        "#     plt.close()\n",
        "\n",
        "#     # Get dimensions\n",
        "#     n_heads = attentions.shape[1]\n",
        "#     h_featmap = img_np.shape[0] // model.patch_size[0]\n",
        "#     w_featmap = img_np.shape[1] // model.patch_size[1]\n",
        "\n",
        "#     # Process attention weights\n",
        "#     attentions = attentions[0]  # First batch\n",
        "#     print(\">>>>>>>\", attentions.shape)\n",
        "#     # We focus on attention from CLS token to patch tokens\n",
        "#     cls_attn = attentions[:, 0, 1:].reshape(n_heads, -1)\n",
        "\n",
        "#     # Apply thresholding to keep only a certain percentage of the mass\n",
        "#     if threshold is not None:\n",
        "#         # Sort attention values\n",
        "#         val, idx = torch.sort(cls_attn, dim=1)\n",
        "#         val /= torch.sum(val, dim=1, keepdim=True)\n",
        "#         cumval = torch.cumsum(val, dim=1)\n",
        "#         th_attn = cumval > (1 - threshold)\n",
        "\n",
        "#         # Reorder to original order\n",
        "#         idx2 = torch.argsort(idx)\n",
        "#         for head in range(n_heads):\n",
        "#             th_attn[head] = th_attn[head][idx2[head]]\n",
        "\n",
        "#         # Reshape to feature map dimensions\n",
        "#         th_attn = th_attn.reshape(n_heads, h_featmap, w_featmap).float()\n",
        "\n",
        "#         # Interpolate to image size\n",
        "#         th_attn = nn.functional.interpolate(\n",
        "#             th_attn.unsqueeze(0), \n",
        "#             scale_factor=model.patch_size[0], \n",
        "#             mode=\"nearest\"\n",
        "#         )[0].cpu().numpy()\n",
        "\n",
        "#     # Reshape and interpolate attention maps to image size\n",
        "#     cls_attn = cls_attn.reshape(n_heads, h_featmap, w_featmap)\n",
        "#     cls_attn = nn.functional.interpolate(\n",
        "#         cls_attn.unsqueeze(0), \n",
        "#         scale_factor=model.patch_size[0], \n",
        "#         mode=\"nearest\"\n",
        "#     )[0].detach().cpu().numpy()\n",
        "\n",
        "#     # Save raw attention heatmaps\n",
        "#     for j in range(n_heads):\n",
        "#         fname = f'{save_dir}/attn_heatmap_head{j}_img{image_idx}.png'\n",
        "#         plt.figure(figsize=(5, 5))\n",
        "#         plt.imshow(cls_attn[j])\n",
        "#         plt.axis('off')\n",
        "#         plt.colorbar()\n",
        "#         plt.savefig(fname)\n",
        "#         plt.close()\n",
        "#         print(f\"{fname} saved.\")\n",
        "\n",
        "#     # Save attention masks with contours\n",
        "#     if threshold is not None:\n",
        "#         for j in range(n_heads):\n",
        "#             fname = f'{save_dir}/attn_mask_th{threshold}_head{j}_img{image_idx}.png'\n",
        "#             display_instances(\n",
        "#                 img_uint8, \n",
        "#                 th_attn[j], \n",
        "#                 fname=fname, \n",
        "#                 blur=False, \n",
        "#                 contour=True, \n",
        "#                 alpha=0.5\n",
        "#             )\n",
        "\n",
        "#     # Create a grid of all attention heads\n",
        "#     fig, axes = plt.subplots(2, n_heads//2, figsize=(15, 8))\n",
        "#     axes = axes.flatten()\n",
        "\n",
        "#     for j in range(n_heads):\n",
        "#         axes[j].imshow(img_np)\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     d_model = 12\n",
        "#     n_classes = 10\n",
        "#     img_size = (32,32)\n",
        "#     patch_size = (2,2)\n",
        "#     n_channels = 3\n",
        "#     n_heads = 6\n",
        "#     n_layers = 6\n",
        "#     batch_size = 128\n",
        "#     epochs = 100\n",
        "#     alpha = 0.005\n",
        "#     model = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers)\n",
        "#     train_vit_model(model)\n",
        "#     for i, data in enumerate(test_loader, 0):\n",
        "#             inputs, labels = data\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             break\n",
        "#     visualize_attention_improved(model,inputs[0].unsqueeze(0), 1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "v10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
