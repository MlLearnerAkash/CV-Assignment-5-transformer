{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e565b6d0",
   "metadata": {},
   "source": [
    "-  **Ref:** [DifferentialAttentionHead](https://medium.com/@AykutCayir34/lets-implement-differential-transformer-paper-0e4499659604)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a119d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05acd032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>validation_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>2.20715</td></tr><tr><td>validation_loss</td><td>2.14251</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vit-patchsize-2-attention_head-10-layer-10</strong> at: <a href='https://wandb.ai/manna1/vit-image-classification/runs/6pn35yyg' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification/runs/6pn35yyg</a><br> View project at: <a href='https://wandb.ai/manna1/vit-image-classification' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification</a><br>Synced 5 W&B file(s), 8 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250417_093911-6pn35yyg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q2/wandb/run-20250417_095224-7hhajof2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manna1/vit-image-classification/runs/7hhajof2' target=\"_blank\">vit-patchsize-2-attention_head-10-layer-10</a></strong> to <a href='https://wandb.ai/manna1/vit-image-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manna1/vit-image-classification' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manna1/vit-image-classification/runs/7hhajof2' target=\"_blank\">https://wandb.ai/manna1/vit-image-classification/runs/7hhajof2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"vit_config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Assign the config values to the corresponding variables\n",
    "d_model   = config[\"d_model\"]\n",
    "n_classes = config[\"n_classes\"]\n",
    "img_size  = config[\"img_size\"]\n",
    "patch_size = config[\"patch_size\"]\n",
    "n_channels = config[\"n_channels\"]\n",
    "n_heads   = config[\"n_heads\"]\n",
    "n_layers  = config[\"n_layers\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "epochs    = config[\"epochs\"]\n",
    "alpha     = config[\"alpha\"]\n",
    "\n",
    "exp_name = f\"vit-patchsize-{patch_size[0]}-attention_head-{n_heads}-layer-{n_layers}\"\n",
    "\n",
    "wandb.init(project = \"vit-image-classification\", name = exp_name)\n",
    "\n",
    "config = {\n",
    "    \"d_model\": d_model,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"img_size\": img_size,\n",
    "    \"patch_size\": patch_size,\n",
    "    \"n_channels\": n_channels,\n",
    "    \"n_heads\": n_heads,\n",
    "    \"n_layers\": n_layers,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"alpha\": alpha\n",
    "}\n",
    "\n",
    "wandb.config.update(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3d7958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "  # T.Resize(img_size),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = CIFAR10(\n",
    "  root=\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dataset\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = CIFAR10(\n",
    "  root=\"/home/akash/ws/cv_assignment/assignment-5-MlLearnerAkash/Q1/dataset\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aa73fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_vit import VisionTransformer\n",
    "\n",
    "\n",
    "def train_transformer(transformer,save_path, criterion, epochs, optimizer):\n",
    "   \n",
    "    # Setup\n",
    "    init_val_loss = np.inf\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, \n",
    "          f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    \n",
    "    # Ensure the model is on the proper device.\n",
    "    transformer.to(device)\n",
    "    \n",
    "    # Training & Validation loop\n",
    "    for epoch in range(epochs):\n",
    "        transformer.train()\n",
    "        training_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = transformer(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "        \n",
    "        avg_loss = training_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Train loss: {avg_loss:.3f}')\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_loss})\n",
    "        \n",
    "        # Validation loop\n",
    "        transformer.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in test_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                val_outputs = transformer(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                validation_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = validation_loss / len(test_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs} - Validation loss: {avg_val_loss:.3f}')\n",
    "        wandb.log({\"validation_loss\": avg_val_loss})\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if avg_val_loss < init_val_loss:\n",
    "            init_val_loss = avg_val_loss\n",
    "            torch.save(transformer.state_dict(), os.path.join(save_path, \"best.pt\"))\n",
    "        \n",
    "        # Log a few sample predictions from the last validation batch.\n",
    "        sample_inputs = val_inputs[:4].detach().cpu()\n",
    "        sample_labels = val_labels[:4].detach().cpu()\n",
    "        sample_outputs = val_outputs[:4].detach().cpu()\n",
    "        _, sample_preds = torch.max(sample_outputs, 1)\n",
    "        \n",
    "        samples = []\n",
    "        for idx in range(len(sample_inputs)):\n",
    "            # Convert image from (C, H, W) to (H, W, C) for plotting.\n",
    "            image_np = sample_inputs[idx].permute(1, 2, 0).numpy()\n",
    "            plt.figure(figsize=(2,2))\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f\"GT: {sample_labels[idx].item()} | Pred: {sample_preds[idx].item()}\")\n",
    "            plt.axis(\"off\")\n",
    "            fig = plt.gcf()\n",
    "            samples.append(wandb.Image(fig))\n",
    "            plt.close(fig)\n",
    "        \n",
    "        wandb.log({\"sample_predictions\": samples, \"epoch\": epoch + 1})\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cadab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 4060 Ti)\n",
      "Epoch 1/10 - Train loss: 2.214\n",
      "Epoch 1/10 - Validation loss: 2.170\n",
      "Epoch 2/10 - Train loss: 2.149\n",
      "Epoch 2/10 - Validation loss: 2.105\n",
      "Epoch 3/10 - Train loss: 2.101\n",
      "Epoch 3/10 - Validation loss: 2.076\n",
      "Epoch 4/10 - Train loss: 2.082\n",
      "Epoch 4/10 - Validation loss: 2.059\n",
      "Epoch 5/10 - Train loss: 2.073\n",
      "Epoch 5/10 - Validation loss: 2.044\n",
      "Epoch 6/10 - Train loss: 2.060\n",
      "Epoch 6/10 - Validation loss: 2.038\n",
      "Epoch 7/10 - Train loss: 2.051\n",
      "Epoch 7/10 - Validation loss: 2.051\n",
      "Epoch 8/10 - Train loss: 2.055\n",
      "Epoch 8/10 - Validation loss: 2.043\n",
      "Epoch 9/10 - Train loss: 2.051\n",
      "Epoch 9/10 - Validation loss: 2.029\n",
      "Epoch 10/10 - Train loss: 2.048\n",
      "Epoch 10/10 - Validation loss: 2.041\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers,10).to(device)\n",
    "\n",
    "save_path = exp_name\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = epochs\n",
    "optimizer = Adam(transformer.parameters(), lr=alpha)\n",
    "train_transformer(transformer = transformer,\n",
    "                    save_path=save_path, \n",
    "                    criterion=criterion, \n",
    "                    epochs=epochs, \n",
    "                    optimizer=optimizer)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f268b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442188c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
